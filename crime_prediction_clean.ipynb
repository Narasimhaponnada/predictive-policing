{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8ce58b7",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n",
    "\n",
    "Import necessary libraries and load the Los Angeles crime dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b42965c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9b69b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 1,004,991 rows, 28 columns\n",
      "\n",
      "Column names:\n",
      "['DR_NO', 'Date Rptd', 'DATE OCC', 'TIME OCC', 'AREA', 'AREA NAME', 'Rpt Dist No', 'Part 1-2', 'Crm Cd', 'Crm Cd Desc', 'Mocodes', 'Vict Age', 'Vict Sex', 'Vict Descent', 'Premis Cd', 'Premis Desc', 'Weapon Used Cd', 'Weapon Desc', 'Status', 'Status Desc', 'Crm Cd 1', 'Crm Cd 2', 'Crm Cd 3', 'Crm Cd 4', 'LOCATION', 'Cross Street', 'LAT', 'LON']\n",
      "\n",
      "First few rows:\n",
      "\n",
      "10 Random rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DR_NO</th>\n",
       "      <th>Date Rptd</th>\n",
       "      <th>DATE OCC</th>\n",
       "      <th>TIME OCC</th>\n",
       "      <th>AREA</th>\n",
       "      <th>AREA NAME</th>\n",
       "      <th>Rpt Dist No</th>\n",
       "      <th>Part 1-2</th>\n",
       "      <th>Crm Cd</th>\n",
       "      <th>Crm Cd Desc</th>\n",
       "      <th>...</th>\n",
       "      <th>Status</th>\n",
       "      <th>Status Desc</th>\n",
       "      <th>Crm Cd 1</th>\n",
       "      <th>Crm Cd 2</th>\n",
       "      <th>Crm Cd 3</th>\n",
       "      <th>Crm Cd 4</th>\n",
       "      <th>LOCATION</th>\n",
       "      <th>Cross Street</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>129061</th>\n",
       "      <td>200216570</td>\n",
       "      <td>10/23/2020 12:00:00 AM</td>\n",
       "      <td>10/01/2020 12:00:00 AM</td>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "      <td>Rampart</td>\n",
       "      <td>249</td>\n",
       "      <td>2</td>\n",
       "      <td>956</td>\n",
       "      <td>LETTERS, LEWD  -  TELEPHONE CALLS, LEWD</td>\n",
       "      <td>...</td>\n",
       "      <td>AA</td>\n",
       "      <td>Adult Arrest</td>\n",
       "      <td>956.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>600    ST PAUL                      AV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.0542</td>\n",
       "      <td>-118.2611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324085</th>\n",
       "      <td>211009787</td>\n",
       "      <td>07/04/2021 12:00:00 AM</td>\n",
       "      <td>07/03/2021 12:00:00 AM</td>\n",
       "      <td>2250</td>\n",
       "      <td>10</td>\n",
       "      <td>West Valley</td>\n",
       "      <td>1011</td>\n",
       "      <td>2</td>\n",
       "      <td>354</td>\n",
       "      <td>THEFT OF IDENTITY</td>\n",
       "      <td>...</td>\n",
       "      <td>AA</td>\n",
       "      <td>Adult Arrest</td>\n",
       "      <td>354.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ARMINTA</td>\n",
       "      <td>WOODLEY</td>\n",
       "      <td>34.2047</td>\n",
       "      <td>-118.5531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120194</th>\n",
       "      <td>201225286</td>\n",
       "      <td>11/26/2020 12:00:00 AM</td>\n",
       "      <td>11/26/2020 12:00:00 AM</td>\n",
       "      <td>1610</td>\n",
       "      <td>12</td>\n",
       "      <td>77th Street</td>\n",
       "      <td>1241</td>\n",
       "      <td>1</td>\n",
       "      <td>230</td>\n",
       "      <td>ASSAULT WITH DEADLY WEAPON, AGGRAVATED ASSAULT</td>\n",
       "      <td>...</td>\n",
       "      <td>IC</td>\n",
       "      <td>Invest Cont</td>\n",
       "      <td>230.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6400    WEST                         BL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.9810</td>\n",
       "      <td>-118.3352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738250</th>\n",
       "      <td>231614576</td>\n",
       "      <td>12/01/2023 12:00:00 AM</td>\n",
       "      <td>10/04/2023 12:00:00 AM</td>\n",
       "      <td>900</td>\n",
       "      <td>16</td>\n",
       "      <td>Foothill</td>\n",
       "      <td>1657</td>\n",
       "      <td>1</td>\n",
       "      <td>440</td>\n",
       "      <td>THEFT PLAIN - PETTY ($950 &amp; UNDER)</td>\n",
       "      <td>...</td>\n",
       "      <td>IC</td>\n",
       "      <td>Invest Cont</td>\n",
       "      <td>440.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10100    HILLHAVEN                    AV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.2523</td>\n",
       "      <td>-118.2898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494997</th>\n",
       "      <td>221007431</td>\n",
       "      <td>04/01/2022 12:00:00 AM</td>\n",
       "      <td>03/31/2022 12:00:00 AM</td>\n",
       "      <td>1830</td>\n",
       "      <td>10</td>\n",
       "      <td>West Valley</td>\n",
       "      <td>1004</td>\n",
       "      <td>1</td>\n",
       "      <td>420</td>\n",
       "      <td>THEFT FROM MOTOR VEHICLE - PETTY ($950 &amp; UNDER)</td>\n",
       "      <td>...</td>\n",
       "      <td>IC</td>\n",
       "      <td>Invest Cont</td>\n",
       "      <td>420.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18300    ROSCOE                       BL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.2208</td>\n",
       "      <td>-118.5317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973179</th>\n",
       "      <td>240207085</td>\n",
       "      <td>02/29/2024 12:00:00 AM</td>\n",
       "      <td>02/29/2024 12:00:00 AM</td>\n",
       "      <td>2100</td>\n",
       "      <td>2</td>\n",
       "      <td>Rampart</td>\n",
       "      <td>275</td>\n",
       "      <td>2</td>\n",
       "      <td>626</td>\n",
       "      <td>INTIMATE PARTNER - SIMPLE ASSAULT</td>\n",
       "      <td>...</td>\n",
       "      <td>IC</td>\n",
       "      <td>Invest Cont</td>\n",
       "      <td>626.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1600 W  11TH                         ST</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.0506</td>\n",
       "      <td>-118.2769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387172</th>\n",
       "      <td>210513939</td>\n",
       "      <td>09/21/2021 12:00:00 AM</td>\n",
       "      <td>09/21/2021 12:00:00 AM</td>\n",
       "      <td>1338</td>\n",
       "      <td>5</td>\n",
       "      <td>Harbor</td>\n",
       "      <td>521</td>\n",
       "      <td>2</td>\n",
       "      <td>930</td>\n",
       "      <td>CRIMINAL THREATS - NO WEAPON DISPLAYED</td>\n",
       "      <td>...</td>\n",
       "      <td>IC</td>\n",
       "      <td>Invest Cont</td>\n",
       "      <td>930.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1800 N  TAPER                        AV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.7614</td>\n",
       "      <td>-118.2989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254314</th>\n",
       "      <td>210213404</td>\n",
       "      <td>08/18/2021 12:00:00 AM</td>\n",
       "      <td>08/18/2021 12:00:00 AM</td>\n",
       "      <td>1400</td>\n",
       "      <td>2</td>\n",
       "      <td>Rampart</td>\n",
       "      <td>233</td>\n",
       "      <td>2</td>\n",
       "      <td>626</td>\n",
       "      <td>INTIMATE PARTNER - SIMPLE ASSAULT</td>\n",
       "      <td>...</td>\n",
       "      <td>IC</td>\n",
       "      <td>Invest Cont</td>\n",
       "      <td>626.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2700    BEVERLY                      BL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.0699</td>\n",
       "      <td>-118.2777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755729</th>\n",
       "      <td>231013989</td>\n",
       "      <td>08/14/2023 12:00:00 AM</td>\n",
       "      <td>08/08/2023 12:00:00 AM</td>\n",
       "      <td>1400</td>\n",
       "      <td>10</td>\n",
       "      <td>West Valley</td>\n",
       "      <td>1005</td>\n",
       "      <td>1</td>\n",
       "      <td>331</td>\n",
       "      <td>THEFT FROM MOTOR VEHICLE - GRAND ($950.01 AND ...</td>\n",
       "      <td>...</td>\n",
       "      <td>IC</td>\n",
       "      <td>Invest Cont</td>\n",
       "      <td>331.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7600    HESPERIA                     AV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.2084</td>\n",
       "      <td>-118.5263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975464</th>\n",
       "      <td>240613742</td>\n",
       "      <td>12/03/2024 12:00:00 AM</td>\n",
       "      <td>11/29/2024 12:00:00 AM</td>\n",
       "      <td>1624</td>\n",
       "      <td>6</td>\n",
       "      <td>Hollywood</td>\n",
       "      <td>637</td>\n",
       "      <td>1</td>\n",
       "      <td>440</td>\n",
       "      <td>THEFT PLAIN - PETTY ($950 &amp; UNDER)</td>\n",
       "      <td>...</td>\n",
       "      <td>IC</td>\n",
       "      <td>Invest Cont</td>\n",
       "      <td>440.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6200    HOLLYWOOD                    BL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.1030</td>\n",
       "      <td>-118.3225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows √ó 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            DR_NO               Date Rptd                DATE OCC  TIME OCC  \\\n",
       "129061  200216570  10/23/2020 12:00:00 AM  10/01/2020 12:00:00 AM      2000   \n",
       "324085  211009787  07/04/2021 12:00:00 AM  07/03/2021 12:00:00 AM      2250   \n",
       "120194  201225286  11/26/2020 12:00:00 AM  11/26/2020 12:00:00 AM      1610   \n",
       "738250  231614576  12/01/2023 12:00:00 AM  10/04/2023 12:00:00 AM       900   \n",
       "494997  221007431  04/01/2022 12:00:00 AM  03/31/2022 12:00:00 AM      1830   \n",
       "973179  240207085  02/29/2024 12:00:00 AM  02/29/2024 12:00:00 AM      2100   \n",
       "387172  210513939  09/21/2021 12:00:00 AM  09/21/2021 12:00:00 AM      1338   \n",
       "254314  210213404  08/18/2021 12:00:00 AM  08/18/2021 12:00:00 AM      1400   \n",
       "755729  231013989  08/14/2023 12:00:00 AM  08/08/2023 12:00:00 AM      1400   \n",
       "975464  240613742  12/03/2024 12:00:00 AM  11/29/2024 12:00:00 AM      1624   \n",
       "\n",
       "        AREA    AREA NAME  Rpt Dist No  Part 1-2  Crm Cd  \\\n",
       "129061     2      Rampart          249         2     956   \n",
       "324085    10  West Valley         1011         2     354   \n",
       "120194    12  77th Street         1241         1     230   \n",
       "738250    16     Foothill         1657         1     440   \n",
       "494997    10  West Valley         1004         1     420   \n",
       "973179     2      Rampart          275         2     626   \n",
       "387172     5       Harbor          521         2     930   \n",
       "254314     2      Rampart          233         2     626   \n",
       "755729    10  West Valley         1005         1     331   \n",
       "975464     6    Hollywood          637         1     440   \n",
       "\n",
       "                                              Crm Cd Desc  ... Status  \\\n",
       "129061            LETTERS, LEWD  -  TELEPHONE CALLS, LEWD  ...     AA   \n",
       "324085                                  THEFT OF IDENTITY  ...     AA   \n",
       "120194     ASSAULT WITH DEADLY WEAPON, AGGRAVATED ASSAULT  ...     IC   \n",
       "738250                 THEFT PLAIN - PETTY ($950 & UNDER)  ...     IC   \n",
       "494997    THEFT FROM MOTOR VEHICLE - PETTY ($950 & UNDER)  ...     IC   \n",
       "973179                  INTIMATE PARTNER - SIMPLE ASSAULT  ...     IC   \n",
       "387172             CRIMINAL THREATS - NO WEAPON DISPLAYED  ...     IC   \n",
       "254314                  INTIMATE PARTNER - SIMPLE ASSAULT  ...     IC   \n",
       "755729  THEFT FROM MOTOR VEHICLE - GRAND ($950.01 AND ...  ...     IC   \n",
       "975464                 THEFT PLAIN - PETTY ($950 & UNDER)  ...     IC   \n",
       "\n",
       "         Status Desc Crm Cd 1 Crm Cd 2  Crm Cd 3 Crm Cd 4  \\\n",
       "129061  Adult Arrest    956.0      NaN       NaN      NaN   \n",
       "324085  Adult Arrest    354.0      NaN       NaN      NaN   \n",
       "120194   Invest Cont    230.0      NaN       NaN      NaN   \n",
       "738250   Invest Cont    440.0      NaN       NaN      NaN   \n",
       "494997   Invest Cont    420.0      NaN       NaN      NaN   \n",
       "973179   Invest Cont    626.0      NaN       NaN      NaN   \n",
       "387172   Invest Cont    930.0      NaN       NaN      NaN   \n",
       "254314   Invest Cont    626.0      NaN       NaN      NaN   \n",
       "755729   Invest Cont    331.0      NaN       NaN      NaN   \n",
       "975464   Invest Cont    440.0      NaN       NaN      NaN   \n",
       "\n",
       "                                        LOCATION Cross Street      LAT  \\\n",
       "129061    600    ST PAUL                      AV          NaN  34.0542   \n",
       "324085                                   ARMINTA      WOODLEY  34.2047   \n",
       "120194   6400    WEST                         BL          NaN  33.9810   \n",
       "738250  10100    HILLHAVEN                    AV          NaN  34.2523   \n",
       "494997  18300    ROSCOE                       BL          NaN  34.2208   \n",
       "973179   1600 W  11TH                         ST          NaN  34.0506   \n",
       "387172   1800 N  TAPER                        AV          NaN  33.7614   \n",
       "254314   2700    BEVERLY                      BL          NaN  34.0699   \n",
       "755729   7600    HESPERIA                     AV          NaN  34.2084   \n",
       "975464   6200    HOLLYWOOD                    BL          NaN  34.1030   \n",
       "\n",
       "             LON  \n",
       "129061 -118.2611  \n",
       "324085 -118.5531  \n",
       "120194 -118.3352  \n",
       "738250 -118.2898  \n",
       "494997 -118.5317  \n",
       "973179 -118.2769  \n",
       "387172 -118.2989  \n",
       "254314 -118.2777  \n",
       "755729 -118.5263  \n",
       "975464 -118.3225  \n",
       "\n",
       "[10 rows x 28 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('Crime_Data_from_2020_to_Present.csv')\n",
    "\n",
    "print(f\"Dataset loaded: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
    "print(f\"\\nColumn names:\")\n",
    "print(df.columns.tolist())\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()\n",
    "\n",
    "print(f\"\\n10 Random rows:\")\n",
    "df.sample(n=10, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf70c7b",
   "metadata": {},
   "source": [
    "## 2. Data Exploration and Cleaning\n",
    "\n",
    "Explore the data structure, check for missing values, and perform basic data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e56960e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Info:\n",
      "Shape: (1004991, 28)\n",
      "\n",
      "Missing values:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Count</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Crm Cd 4</th>\n",
       "      <td>1004927</td>\n",
       "      <td>99.993632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Crm Cd 3</th>\n",
       "      <td>1002677</td>\n",
       "      <td>99.769749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Crm Cd 2</th>\n",
       "      <td>935831</td>\n",
       "      <td>93.118346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cross Street</th>\n",
       "      <td>850755</td>\n",
       "      <td>84.652997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weapon Desc</th>\n",
       "      <td>677744</td>\n",
       "      <td>67.437818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weapon Used Cd</th>\n",
       "      <td>677744</td>\n",
       "      <td>67.437818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mocodes</th>\n",
       "      <td>151619</td>\n",
       "      <td>15.086603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vict Descent</th>\n",
       "      <td>144656</td>\n",
       "      <td>14.393761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vict Sex</th>\n",
       "      <td>144644</td>\n",
       "      <td>14.392567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Premis Desc</th>\n",
       "      <td>588</td>\n",
       "      <td>0.058508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Premis Cd</th>\n",
       "      <td>16</td>\n",
       "      <td>0.001592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Crm Cd 1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.001095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Status</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Missing Count  Percentage\n",
       "Crm Cd 4              1004927   99.993632\n",
       "Crm Cd 3              1002677   99.769749\n",
       "Crm Cd 2               935831   93.118346\n",
       "Cross Street           850755   84.652997\n",
       "Weapon Desc            677744   67.437818\n",
       "Weapon Used Cd         677744   67.437818\n",
       "Mocodes                151619   15.086603\n",
       "Vict Descent           144656   14.393761\n",
       "Vict Sex               144644   14.392567\n",
       "Premis Desc               588    0.058508\n",
       "Premis Cd                  16    0.001592\n",
       "Crm Cd 1                   11    0.001095\n",
       "Status                      1    0.000100"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic data information\n",
    "print(\"Dataset Info:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nMissing values:\")\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (missing_data / len(df)) * 100\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing Count': missing_data,\n",
    "    'Percentage': missing_percent\n",
    "})\n",
    "missing_summary[missing_summary['Missing Count'] > 0].sort_values('Missing Count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fbd16a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìä SMART DATA CLEANING STRATEGY\n",
      "================================================================================\n",
      "üóÇÔ∏è  ORIGINAL DATASET: 1,004,991 total records\n",
      "\n",
      "üéØ CORE FEATURES REQUIRED:\n",
      "   District: Need at least one of ['AREA', 'AREA NAME']\n",
      "   Time: DATE OCC (when crime occurred)\n",
      "   Crime Type: Crm Cd Desc (what happened)\n",
      "\n",
      "üìâ MISSING DATA ANALYSIS:\n",
      "   District info missing:        0 records (  0.0%)\n",
      "   Occurrence date missing:      0 records (  0.0%)\n",
      "   Crime type missing:         0 records (  0.0%)\n",
      "   ANY core feature missing:     0 records (  0.0%)\n",
      "\n",
      "üìç OPTIONAL FEATURES ANALYSIS:\n",
      "   Coordinates missing:          0 records (  0.0%)\n",
      "   Coordinates invalid:      2,240 records (  0.2%)\n",
      "   Exact time missing:           0 records (  0.0%)\n",
      "\n",
      "üìä STRATEGY COMPARISON:\n",
      "   Old Strategy (Remove missing coordinates):\n",
      "     Records kept:    1,002,751 ( 99.8%)\n",
      "     Records lost:        2,240 (  0.2%)\n",
      "\n",
      "   New Strategy (Keep core features):\n",
      "     Records kept:    1,004,991 (100.0%)\n",
      "     Records lost:            0 (  0.0%)\n",
      "\n",
      "   üìà IMPROVEMENT:        2,240 additional records (  0.2% more data)\n",
      "\n",
      "üßπ APPLYING SMART CLEANING:\n",
      "   ‚úÖ Keeping records with missing coordinates but complete core features\n",
      "   ‚úÖ Keeping records with missing exact time but have date\n",
      "   ‚ùå Removing ONLY records missing essential crime information\n",
      "   Removed: 0 records missing core features\n",
      "   Retained: 1,004,991 records (100.0%)\n",
      "\n",
      "üîß CREATING MISSING DATA INDICATORS:\n",
      "   ‚úÖ has_coordinates: 1,004,991 records\n",
      "   ‚úÖ has_valid_coordinates: 1,002,751 records\n",
      "   ‚úÖ has_exact_time: 1,004,991 records\n",
      "\n",
      "üîÑ DUPLICATE REMOVAL:\n",
      "   Removed: 0 duplicate records\n",
      "   Final dataset: 1,004,991 records\n",
      "\n",
      "üíé VALUABLE RECORDS SAVED: 2,240\n",
      "   These records have:\n",
      "   ‚úÖ District information ‚Üí Can do district-level analysis\n",
      "   ‚úÖ Crime occurrence date ‚Üí Can do temporal analysis\n",
      "   ‚úÖ Crime type ‚Üí Can do crime category analysis\n",
      "   ‚ùå Missing/invalid coordinates ‚Üí Will impute with district centroids\n",
      "\n",
      "   üìù Examples of saved records:\n",
      "      Crime: ARSON\n",
      "      District: N Hollywood\n",
      "      Date: 11/20/2020 12:00:00 AM\n",
      "      Coords: LAT=0.0, LON=0.0\n",
      "      ---\n",
      "      Crime: TRESPASSING\n",
      "      District: Pacific\n",
      "      Date: 05/06/2020 12:00:00 AM\n",
      "      Coords: LAT=0.0, LON=0.0\n",
      "      ---\n",
      "      Crime: THEFT FROM MOTOR VEHICLE - GRAND ($950.0\n",
      "      District: Wilshire\n",
      "      Date: 12/13/2020 12:00:00 AM\n",
      "      Coords: LAT=0.0, LON=0.0\n",
      "      ---\n",
      "\n",
      "‚úÖ SMART CLEANING COMPLETE!\n",
      "üìä FINAL DATASET: 1,004,991 records\n",
      "üìà DATA RETENTION: 100.0% of original data\n",
      "üéØ STRATEGY: Focus on core features, handle missing data intelligently\n",
      "üí° NEXT: Will impute missing coordinates using district averages\n",
      "\n",
      "üìã CLEANED DATASET SUMMARY:\n",
      "   Records with valid coordinates: 1,002,751 (99.8%)\n",
      "   Records with exact time: 1,004,991 (100.0%)\n",
      "   Records with complete data: 1,002,751 (99.8%)\n",
      "   Average data completeness: 0.999\n"
     ]
    }
   ],
   "source": [
    "# SMART DATA CLEANING - Core Features Strategy\n",
    "print(\"=\"*80)\n",
    "print(\"üìä SMART DATA CLEANING STRATEGY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"üóÇÔ∏è  ORIGINAL DATASET: {len(df):,} total records\")\n",
    "\n",
    "# Define core features essential for predictive policing\n",
    "core_features_required = {\n",
    "    'district': ['AREA', 'AREA NAME'],    # At least one district identifier\n",
    "    'time': ['DATE OCC'],                 # Crime occurrence date\n",
    "    'crime_type': ['Crm Cd Desc']         # Crime description\n",
    "}\n",
    "\n",
    "print(f\"\\nüéØ CORE FEATURES REQUIRED:\")\n",
    "print(f\"   District: Need at least one of {core_features_required['district']}\")\n",
    "print(f\"   Time: {core_features_required['time'][0]} (when crime occurred)\")\n",
    "print(f\"   Crime Type: {core_features_required['crime_type'][0]} (what happened)\")\n",
    "\n",
    "# Analyze missing data for core features\n",
    "print(f\"\\nüìâ MISSING DATA ANALYSIS:\")\n",
    "\n",
    "# District missing (need at least one district identifier)\n",
    "district_missing = (df['AREA'].isnull() & df['AREA NAME'].isnull())\n",
    "print(f\"   District info missing: {district_missing.sum():>8,} records ({district_missing.sum()/len(df)*100:>5.1f}%)\")\n",
    "\n",
    "# Time missing\n",
    "time_missing = df['DATE OCC'].isnull()\n",
    "print(f\"   Occurrence date missing: {time_missing.sum():>6,} records ({time_missing.sum()/len(df)*100:>5.1f}%)\")\n",
    "\n",
    "# Crime type missing\n",
    "crime_type_missing = df['Crm Cd Desc'].isnull()\n",
    "print(f\"   Crime type missing: {crime_type_missing.sum():>9,} records ({crime_type_missing.sum()/len(df)*100:>5.1f}%)\")\n",
    "\n",
    "# Combined core missing (remove only these)\n",
    "core_missing = district_missing | time_missing | crime_type_missing\n",
    "print(f\"   ANY core feature missing: {core_missing.sum():>5,} records ({core_missing.sum()/len(df)*100:>5.1f}%)\")\n",
    "\n",
    "# Optional features analysis (don't remove for these)\n",
    "print(f\"\\nüìç OPTIONAL FEATURES ANALYSIS:\")\n",
    "coord_missing = df['LAT'].isnull() | df['LON'].isnull()\n",
    "coord_invalid = (\n",
    "    (df['LAT'] == 0) | (df['LON'] == 0) |\n",
    "    (df['LAT'] < 33) | (df['LAT'] > 35) |\n",
    "    (df['LON'] > -117) | (df['LON'] < -119)\n",
    ")\n",
    "time_occ_missing = df['TIME OCC'].isnull()\n",
    "\n",
    "print(f\"   Coordinates missing: {coord_missing.sum():>10,} records ({coord_missing.sum()/len(df)*100:>5.1f}%)\")\n",
    "print(f\"   Coordinates invalid: {coord_invalid.sum():>10,} records ({coord_invalid.sum()/len(df)*100:>5.1f}%)\")\n",
    "print(f\"   Exact time missing: {time_occ_missing.sum():>11,} records ({time_occ_missing.sum()/len(df)*100:>5.1f}%)\")\n",
    "\n",
    "# Show what we'd lose with old vs new strategy\n",
    "old_strategy_removed = coord_missing | coord_invalid\n",
    "old_kept = len(df) - old_strategy_removed.sum()\n",
    "new_kept = len(df) - core_missing.sum()\n",
    "\n",
    "print(f\"\\nüìä STRATEGY COMPARISON:\")\n",
    "print(f\"   Old Strategy (Remove missing coordinates):\")\n",
    "print(f\"     Records kept: {old_kept:>12,} ({old_kept/len(df)*100:>5.1f}%)\")\n",
    "print(f\"     Records lost: {old_strategy_removed.sum():>12,} ({old_strategy_removed.sum()/len(df)*100:>5.1f}%)\")\n",
    "print(f\"\\n   New Strategy (Keep core features):\")\n",
    "print(f\"     Records kept: {new_kept:>12,} ({new_kept/len(df)*100:>5.1f}%)\")\n",
    "print(f\"     Records lost: {core_missing.sum():>12,} ({core_missing.sum()/len(df)*100:>5.1f}%)\")\n",
    "print(f\"\\n   üìà IMPROVEMENT: {new_kept - old_kept:>12,} additional records ({(new_kept - old_kept)/len(df)*100:>5.1f}% more data)\")\n",
    "\n",
    "# Apply the smart cleaning strategy\n",
    "print(f\"\\nüßπ APPLYING SMART CLEANING:\")\n",
    "print(f\"   ‚úÖ Keeping records with missing coordinates but complete core features\")\n",
    "print(f\"   ‚úÖ Keeping records with missing exact time but have date\")\n",
    "print(f\"   ‚ùå Removing ONLY records missing essential crime information\")\n",
    "\n",
    "# Remove only records missing core features\n",
    "df_clean = df[~core_missing].copy()\n",
    "print(f\"   Removed: {core_missing.sum():,} records missing core features\")\n",
    "print(f\"   Retained: {len(df_clean):,} records ({len(df_clean)/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Create missing data indicators as features (valuable information!)\n",
    "print(f\"\\nüîß CREATING MISSING DATA INDICATORS:\")\n",
    "\n",
    "# Coordinate availability\n",
    "df_clean['has_coordinates'] = ~(df_clean['LAT'].isnull() | df_clean['LON'].isnull())\n",
    "df_clean['has_valid_coordinates'] = (\n",
    "    df_clean['has_coordinates'] & \n",
    "    (df_clean['LAT'] != 0) & (df_clean['LON'] != 0) &\n",
    "    (df_clean['LAT'] > 33) & (df_clean['LAT'] < 35) &\n",
    "    (df_clean['LON'] > -119) & (df_clean['LON'] < -117)\n",
    ")\n",
    "\n",
    "# Time availability\n",
    "df_clean['has_exact_time'] = ~df_clean['TIME OCC'].isnull()\n",
    "\n",
    "# Data completeness score\n",
    "df_clean['data_completeness_score'] = (\n",
    "    df_clean['has_valid_coordinates'].astype(int) +\n",
    "    df_clean['has_exact_time'].astype(int) +\n",
    "    (~df_clean['Vict Age'].isnull()).astype(int)\n",
    ") / 3\n",
    "\n",
    "print(f\"   ‚úÖ has_coordinates: {df_clean['has_coordinates'].sum():,} records\")\n",
    "print(f\"   ‚úÖ has_valid_coordinates: {df_clean['has_valid_coordinates'].sum():,} records\")\n",
    "print(f\"   ‚úÖ has_exact_time: {df_clean['has_exact_time'].sum():,} records\")\n",
    "\n",
    "# Remove duplicates (still important)\n",
    "initial_clean_count = len(df_clean)\n",
    "df_clean = df_clean.drop_duplicates()\n",
    "duplicates_removed = initial_clean_count - len(df_clean)\n",
    "print(f\"\\nüîÑ DUPLICATE REMOVAL:\")\n",
    "print(f\"   Removed: {duplicates_removed:,} duplicate records\")\n",
    "print(f\"   Final dataset: {len(df_clean):,} records\")\n",
    "\n",
    "# Show what types of records we're keeping that would have been lost\n",
    "records_saved = (coord_missing | coord_invalid) & ~core_missing\n",
    "if records_saved.sum() > 0:\n",
    "    print(f\"\\nüíé VALUABLE RECORDS SAVED: {records_saved.sum():,}\")\n",
    "    print(f\"   These records have:\")\n",
    "    print(f\"   ‚úÖ District information ‚Üí Can do district-level analysis\")\n",
    "    print(f\"   ‚úÖ Crime occurrence date ‚Üí Can do temporal analysis\") \n",
    "    print(f\"   ‚úÖ Crime type ‚Üí Can do crime category analysis\")\n",
    "    print(f\"   ‚ùå Missing/invalid coordinates ‚Üí Will impute with district centroids\")\n",
    "    \n",
    "    # Show examples\n",
    "    saved_examples = df[records_saved].head(3)\n",
    "    print(f\"\\n   üìù Examples of saved records:\")\n",
    "    for idx, row in saved_examples.iterrows():\n",
    "        print(f\"      Crime: {str(row.get('Crm Cd Desc', 'Unknown'))[:40]}\")\n",
    "        print(f\"      District: {row.get('AREA NAME', 'Unknown')}\")\n",
    "        print(f\"      Date: {row.get('DATE OCC', 'Unknown')}\")\n",
    "        print(f\"      Coords: LAT={row.get('LAT', 'Missing')}, LON={row.get('LON', 'Missing')}\")\n",
    "        print(f\"      ---\")\n",
    "\n",
    "# Update df to cleaned version\n",
    "df = df_clean.copy()\n",
    "\n",
    "print(f\"\\n‚úÖ SMART CLEANING COMPLETE!\")\n",
    "print(f\"üìä FINAL DATASET: {len(df):,} records\")\n",
    "print(f\"üìà DATA RETENTION: {len(df)/len(df_clean)*100:.1f}% of original data\")\n",
    "print(f\"üéØ STRATEGY: Focus on core features, handle missing data intelligently\")\n",
    "print(f\"üí° NEXT: Will impute missing coordinates using district averages\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nüìã CLEANED DATASET SUMMARY:\")\n",
    "print(f\"   Records with valid coordinates: {df['has_valid_coordinates'].sum():,} ({df['has_valid_coordinates'].mean()*100:.1f}%)\")\n",
    "print(f\"   Records with exact time: {df['has_exact_time'].sum():,} ({df['has_exact_time'].mean()*100:.1f}%)\")\n",
    "print(f\"   Records with complete data: {(df['data_completeness_score'] == 1).sum():,} ({(df['data_completeness_score'] == 1).mean()*100:.1f}%)\")\n",
    "print(f\"   Average data completeness: {df['data_completeness_score'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3aeb55",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Create time-based and categorical features that will be available before a crime occurs (no data leakage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b1f4495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time features created successfully (CORRECTED)\n",
      "Date range: 2020-01-01 00:00:00 to 2025-05-29 00:00:00\n",
      "Valid times: 1,004,991 (100.0%)\n",
      "Missing/invalid times: 0 (0.0%)\n",
      "\n",
      "Top 10 crime hours:\n",
      "   1. 12:00 - 67,813 crimes\n",
      "   2. 18:00 - 59,958 crimes\n",
      "   3. 17:00 - 58,811 crimes\n",
      "   4. 20:00 - 56,350 crimes\n",
      "   5. 19:00 - 55,597 crimes\n",
      "   6. 16:00 - 52,976 crimes\n",
      "   7. 15:00 - 52,824 crimes\n",
      "   8. 21:00 - 50,793 crimes\n",
      "   9. 14:00 - 49,301 crimes\n",
      "  10. 22:00 - 49,103 crimes\n"
     ]
    }
   ],
   "source": [
    "# Parse date and time information\n",
    "df['Date Rptd'] = pd.to_datetime(df['Date Rptd'], errors='coerce')\n",
    "df['DATE OCC'] = pd.to_datetime(df['DATE OCC'], errors='coerce')\n",
    "\n",
    "# Extract time features from occurrence date\n",
    "df['year'] = df['DATE OCC'].dt.year\n",
    "df['month'] = df['DATE OCC'].dt.month\n",
    "df['day_of_week'] = df['DATE OCC'].dt.dayofweek\n",
    "\n",
    "# CORRECTED: Extract hour from TIME OCC (format is HHMM, e.g., 1230 = 12:30)\n",
    "# Convert TIME OCC to numeric and divide by 100 to get the hour\n",
    "df['TIME_OCC_numeric'] = pd.to_numeric(df['TIME OCC'], errors='coerce')\n",
    "\n",
    "# Validate time ranges (0000 to 2359 with minutes < 60)\n",
    "valid_time_mask = (\n",
    "    (df['TIME_OCC_numeric'] >= 0) & \n",
    "    (df['TIME_OCC_numeric'] <= 2359) &\n",
    "    ((df['TIME_OCC_numeric'] % 100) < 60)  # Minutes must be < 60\n",
    ")\n",
    "\n",
    "# Extract hour correctly: divide by 100 to get hour portion\n",
    "df['hour'] = np.where(\n",
    "    valid_time_mask,\n",
    "    (df['TIME_OCC_numeric'] // 100).astype(int),\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# Create missing time indicator (useful feature - knowing time is missing is information)\n",
    "df['time_unknown'] = df['hour'].isnull().astype(int)\n",
    "\n",
    "# Create additional time features\n",
    "df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "df['is_night'] = ((df['hour'] >= 18) | (df['hour'] <= 6)).astype(int)\n",
    "\n",
    "# Fill NaN in is_night for unknown times (conservative: assume not night)\n",
    "df['is_night'] = df['is_night'].fillna(0).astype(int)\n",
    "\n",
    "print(\"Time features created successfully (CORRECTED)\")\n",
    "print(f\"Date range: {df['DATE OCC'].min()} to {df['DATE OCC'].max()}\")\n",
    "print(f\"Valid times: {valid_time_mask.sum():,} ({valid_time_mask.sum()/len(df)*100:.1f}%)\")\n",
    "print(f\"Missing/invalid times: {(~valid_time_mask).sum():,} ({(~valid_time_mask).sum()/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Show hour distribution to verify it makes sense\n",
    "hour_dist = df['hour'].value_counts().sort_index()\n",
    "print(f\"\\nTop 10 crime hours:\")\n",
    "for i, (hour, count) in enumerate(hour_dist.nlargest(10).items(), 1):\n",
    "    print(f\"  {i:2d}. {int(hour):2d}:00 - {count:>6,} crimes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d975754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Vict Sex: 6 unique values\n",
      "Encoded Vict Descent: 21 unique values\n",
      "Encoded AREA NAME: 21 unique values\n",
      "\n",
      "Categorical encoding completed\n"
     ]
    }
   ],
   "source": [
    "# Encode categorical variables (only non-leaky features)\n",
    "categorical_features = ['Vict Sex', 'Vict Descent', 'AREA NAME']\n",
    "label_encoders = {}\n",
    "\n",
    "for feature in categorical_features:\n",
    "    if feature in df.columns:\n",
    "        # Fill missing values with 'Unknown'\n",
    "        df[feature] = df[feature].fillna('Unknown')\n",
    "        \n",
    "        # Create label encoder\n",
    "        le = LabelEncoder()\n",
    "        df[f'{feature}_encoded'] = le.fit_transform(df[feature])\n",
    "        label_encoders[feature] = le\n",
    "        \n",
    "        print(f\"Encoded {feature}: {len(le.classes_)} unique values\")\n",
    "\n",
    "# Fill victim age with median\n",
    "df['Vict Age'] = pd.to_numeric(df['Vict Age'], errors='coerce')\n",
    "df['Vict Age'] = df['Vict Age'].fillna(df['Vict Age'].median())\n",
    "\n",
    "print(\"\\nCategorical encoding completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14ed2e1",
   "metadata": {},
   "source": [
    "## 4. Target Variable Creation\n",
    "\n",
    "Create a multi-class target variable for detailed crime type prediction based on the top 20 most common crime types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26510f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze crime type distribution for multi-class classification\n",
    "print(\"=== CRIME TYPE ANALYSIS FOR CLASSIFICATION ===\")\n",
    "\n",
    "if 'Crm Cd Desc' in df.columns:\n",
    "    # Get crime type counts\n",
    "    crime_counts = df['Crm Cd Desc'].value_counts()\n",
    "    print(f\"Total unique crime types: {len(crime_counts)}\")\n",
    "    \n",
    "    # Show top 20 crime types\n",
    "    print(f\"\\nTop 20 Crime Types:\")\n",
    "    for i, (crime, count) in enumerate(crime_counts.head(20).items(), 1):\n",
    "        print(f\"{i:2d}. {crime:<50} {count:>8,} ({count/len(df)*100:5.2f}%)\")\n",
    "    \n",
    "    # Create multi-class target variable\n",
    "    top_20_crimes = crime_counts.head(20).index.tolist()\n",
    "    \n",
    "    # Create categorical target\n",
    "    df['crime_category'] = df['Crm Cd Desc'].apply(\n",
    "        lambda x: x if x in top_20_crimes else 'OTHER'\n",
    "    )\n",
    "    \n",
    "    # Display new target distribution\n",
    "    category_dist = df['crime_category'].value_counts()\n",
    "    print(f\"\\n=== NEW TARGET VARIABLE DISTRIBUTION ===\")\n",
    "    print(f\"Total Categories: {len(category_dist)} (Top 20 + OTHER)\")\n",
    "    print(f\"\\nCategory Distribution:\")\n",
    "    for i, (category, count) in enumerate(category_dist.items(), 1):\n",
    "        print(f\"{i:2d}. {category:<50} {count:>8,} ({count/len(df)*100:5.2f}%)\")\n",
    "    \n",
    "    # Visualize the distribution\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # Top 20 crimes detailed view\n",
    "    plt.subplot(2, 2, 1)\n",
    "    top_20_dist = category_dist[category_dist.index != 'OTHER'].head(20)\n",
    "    plt.barh(range(len(top_20_dist)), top_20_dist.values)\n",
    "    plt.yticks(range(len(top_20_dist)), [crime[:35] + '...' if len(crime) > 35 else crime for crime in top_20_dist.index])\n",
    "    plt.xlabel('Count')\n",
    "    plt.title('Top 20 Crime Categories (Detailed)')\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    # Overall distribution pie chart\n",
    "    plt.subplot(2, 2, 2)\n",
    "    other_count = category_dist.get('OTHER', 0)\n",
    "    top_20_total = category_dist.drop('OTHER', errors='ignore').sum()\n",
    "    \n",
    "    plt.pie([top_20_total, other_count], \n",
    "            labels=[f'Top 20 Categories\\n({top_20_total:,})', f'OTHER\\n({other_count:,})'], \n",
    "            autopct='%1.1f%%', startangle=90, colors=['lightblue', 'lightgray'])\n",
    "    plt.title('Crime Distribution: Top 20 vs Others')\n",
    "    \n",
    "    # Top 10 for better readability\n",
    "    plt.subplot(2, 2, 3)\n",
    "    top_10_dist = category_dist.head(10)\n",
    "    plt.bar(range(len(top_10_dist)), top_10_dist.values, color='steelblue', alpha=0.8)\n",
    "    plt.xticks(range(len(top_10_dist)), [crime[:15] + '...' if len(crime) > 15 else crime for crime in top_10_dist.index], rotation=45, ha='right')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Top 10 Crime Categories')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Class balance analysis\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    classification_summary = f\"\"\"\n",
    "MULTI-CLASS CLASSIFICATION SUMMARY\n",
    "\n",
    "Total Records: {len(df):,}\n",
    "Number of Classes: {len(category_dist)}\n",
    "\n",
    "Class Distribution:\n",
    "  Most Common: {category_dist.index[0]}\n",
    "  Count: {category_dist.iloc[0]:,} ({category_dist.iloc[0]/len(df)*100:.1f}%)\n",
    "  \n",
    "  Least Common: {category_dist.index[-1]}\n",
    "  Count: {category_dist.iloc[-1]:,} ({category_dist.iloc[-1]/len(df)*100:.1f}%)\n",
    "  \n",
    "Class Imbalance Ratio: {category_dist.iloc[0] / category_dist.iloc[-1]:.1f}:1\n",
    "\n",
    "Benefits of Multi-Class Approach:\n",
    "‚Ä¢ Specific crime type predictions\n",
    "‚Ä¢ Targeted prevention strategies  \n",
    "‚Ä¢ Resource allocation per crime type\n",
    "‚Ä¢ Actionable insights for police\n",
    "‚Ä¢ Better understanding of crime patterns\n",
    "\"\"\"\n",
    "    \n",
    "    plt.text(0.1, 0.9, classification_summary, transform=plt.gca().transAxes, \n",
    "             fontsize=11, verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\", alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nAdvantages of Multi-Class Crime Classification:\")\n",
    "    print(f\"‚úì More actionable insights for specific crime prevention\")\n",
    "    print(f\"‚úì Better resource allocation per crime type\")\n",
    "    print(f\"‚úì Targeted patrol strategies for different crime categories\")\n",
    "    print(f\"‚úì More nuanced understanding of crime patterns\")\n",
    "    print(f\"‚úì Crime-specific temporal and geographic analysis\")\n",
    "\n",
    "else:\n",
    "    print(\"Crime description column not found. Using binary classification.\")\n",
    "    df['crime_category'] = (df['Part 1-2'] == 1).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e522a4",
   "metadata": {},
   "source": [
    "## 5. Feature Selection\n",
    "\n",
    "Select only features that would be available BEFORE a crime occurs to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563cfd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select predictive features (available before crime occurs)\n",
    "predictive_features = [\n",
    "    'hour',                    # Time of day\n",
    "    'day_of_week',            # Day of week\n",
    "    'month',                  # Month\n",
    "    'year',                   # Year\n",
    "    'is_weekend',             # Weekend indicator\n",
    "    'is_night',               # Night time indicator\n",
    "    'LAT',                    # Latitude\n",
    "    'LON',                    # Longitude\n",
    "    'AREA',                   # Area code\n",
    "    'Rpt Dist No',           # Reporting district\n",
    "    'Vict Age',              # Victim age\n",
    "    'Vict Sex_encoded',      # Victim sex\n",
    "    'Vict Descent_encoded',  # Victim ethnicity\n",
    "    'AREA NAME_encoded'      # Area name\n",
    "]\n",
    "\n",
    "# Check which features are available in the dataset\n",
    "available_features = []\n",
    "for feature in predictive_features:\n",
    "    if feature in df.columns:\n",
    "        available_features.append(feature)\n",
    "    else:\n",
    "        print(f\"Feature '{feature}' not found in dataset\")\n",
    "\n",
    "print(f\"\\nUsing {len(available_features)} predictive features:\")\n",
    "for i, feature in enumerate(available_features, 1):\n",
    "    print(f\"{i:2d}. {feature}\")\n",
    "\n",
    "# Create final dataset with multi-class target\n",
    "X = df[available_features].copy()\n",
    "y = df['crime_category'].copy()\n",
    "\n",
    "# Handle any remaining missing values\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "# Encode the multi-class target variable\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(f\"\\nFinal dataset: {X.shape[0]:,} samples, {X.shape[1]} features\")\n",
    "print(f\"Target classes: {len(label_encoder.classes_)} crime categories\")\n",
    "print(f\"Class distribution:\")\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    count = np.sum(y_encoded == i)\n",
    "    print(f\"  {i:2d}. {class_name:<40} {count:>8,} ({count/len(y_encoded)*100:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408dd6ab",
   "metadata": {},
   "source": [
    "## 6. Data Splitting\n",
    "\n",
    "Split the data into training, validation, and test sets using temporal ordering for realistic evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c20894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporal splits (sorted by year for realistic time-based validation)\n",
    "df_combined = X.copy()\n",
    "df_combined['target'] = y_encoded  # Use encoded target for multi-class\n",
    "df_combined = df_combined.sort_values('year')\n",
    "\n",
    "# Split ratios: 60% train, 20% validation, 20% test\n",
    "n_total = len(df_combined)\n",
    "n_train = int(0.6 * n_total)\n",
    "n_val = int(0.2 * n_total)\n",
    "\n",
    "# Create splits\n",
    "X_train = df_combined.iloc[:n_train][available_features]\n",
    "y_train = df_combined.iloc[:n_train]['target']\n",
    "\n",
    "X_val = df_combined.iloc[n_train:n_train+n_val][available_features]\n",
    "y_val = df_combined.iloc[n_train:n_train+n_val]['target']\n",
    "\n",
    "X_test = df_combined.iloc[n_train+n_val:][available_features]\n",
    "y_test = df_combined.iloc[n_train+n_val:]['target']\n",
    "\n",
    "print(f\"Training set: {len(X_train):,} samples\")\n",
    "print(f\"Validation set: {len(X_val):,} samples\")\n",
    "print(f\"Test set: {len(X_test):,} samples\")\n",
    "\n",
    "# Check class distribution in each set\n",
    "print(f\"\\nClass distribution across splits:\")\n",
    "print(f\"Number of classes: {len(label_encoder.classes_)}\")\n",
    "\n",
    "# Show top 5 classes distribution across splits\n",
    "for split_name, y_split in [(\"Train\", y_train), (\"Val\", y_val), (\"Test\", y_test)]:\n",
    "    class_dist = pd.Series(y_split).value_counts().sort_index()\n",
    "    print(f\"\\n{split_name} set - Top 5 classes:\")\n",
    "    for i in range(min(5, len(class_dist))):\n",
    "        class_idx = class_dist.index[i]\n",
    "        class_name = label_encoder.classes_[class_idx]\n",
    "        count = class_dist.iloc[i]\n",
    "        print(f\"  {class_name:<40} {count:>6,} ({count/len(y_split)*100:4.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f731d6",
   "metadata": {},
   "source": [
    "## 7. Multi-Class Model Training\n",
    "\n",
    "Train an XGBoost classifier with multi-class objective for detailed crime type prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f58e4ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label_encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Configure XGBoost model for multi-class classification\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m num_classes = \u001b[38;5;28mlen\u001b[39m(\u001b[43mlabel_encoder\u001b[49m.classes_)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConfiguring XGBoost for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m crime categories...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m model = XGBClassifier(\n\u001b[32m      6\u001b[39m     objective=\u001b[33m'\u001b[39m\u001b[33mmulti:softprob\u001b[39m\u001b[33m'\u001b[39m,  \u001b[38;5;66;03m# Multi-class classification with probabilities\u001b[39;00m\n\u001b[32m      7\u001b[39m     num_class=num_classes,\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     eval_metric=\u001b[33m'\u001b[39m\u001b[33mmlogloss\u001b[39m\u001b[33m'\u001b[39m  \u001b[38;5;66;03m# Multi-class log loss\u001b[39;00m\n\u001b[32m     18\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'label_encoder' is not defined"
     ]
    }
   ],
   "source": [
    "# Configure XGBoost model for multi-class classification\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"Configuring XGBoost for {num_classes} crime categories...\")\n",
    "\n",
    "model = XGBClassifier(\n",
    "    objective='multi:softprob',  # Multi-class classification with probabilities\n",
    "    num_class=num_classes,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=150,  # Increased for better multi-class performance\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=1,\n",
    "    reg_lambda=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    eval_metric='mlogloss'  # Multi-class log loss\n",
    ")\n",
    "\n",
    "print(\"Training XGBoost multi-class model...\")\n",
    "model.fit(X_train, y_train, \n",
    "          eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "          verbose=False)\n",
    "print(\"Multi-class model training completed\")\n",
    "\n",
    "# Display class mapping for reference\n",
    "print(f\"\\nClass Mapping:\")\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    print(f\"  {i:2d}: {class_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8468a428",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation\n",
    "\n",
    "Evaluate the model performance on validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4928ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make multi-class predictions\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_val_pred = model.predict(X_val)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Get prediction probabilities for detailed analysis\n",
    "y_train_proba = model.predict_proba(X_train)\n",
    "y_val_proba = model.predict_proba(X_val)\n",
    "y_test_proba = model.predict_proba(X_test)\n",
    "\n",
    "print(\"Multi-class predictions completed\")\n",
    "print(f\"Prediction shape: {y_test_pred.shape}\")\n",
    "print(f\"Probability shape: {y_test_proba.shape}\")\n",
    "\n",
    "# Show sample predictions with confidence\n",
    "print(f\"\\nSample predictions with confidence:\")\n",
    "sample_indices = np.random.choice(len(y_test), 5, replace=False)\n",
    "for idx in sample_indices:\n",
    "    true_class = label_encoder.classes_[y_test.iloc[idx]]\n",
    "    pred_class = label_encoder.classes_[y_test_pred[idx]]\n",
    "    confidence = y_test_proba[idx].max()\n",
    "    print(f\"  True: {true_class:<35} | Pred: {pred_class:<35} | Conf: {confidence:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c393e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate multi-class model performance\n",
    "def evaluate_multiclass_model(y_true, y_pred, y_proba, dataset_name):\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"\\n{dataset_name} Performance:\")\n",
    "    print(f\"  Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"  Weighted Precision: {precision:.3f}\")\n",
    "    print(f\"  Weighted Recall: {recall:.3f}\")\n",
    "    print(f\"  Weighted F1-Score: {f1:.3f}\")\n",
    "    \n",
    "    # Top-3 accuracy (useful for multi-class)\n",
    "    top3_accuracy = 0\n",
    "    for i, (true_class, pred_probs) in enumerate(zip(y_true, y_proba)):\n",
    "        top3_pred = np.argsort(pred_probs)[-3:]  # Top 3 predictions\n",
    "        if true_class in top3_pred:\n",
    "            top3_accuracy += 1\n",
    "    top3_accuracy /= len(y_true)\n",
    "    \n",
    "    print(f\"  Top-3 Accuracy: {top3_accuracy:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy, \n",
    "        'precision': precision, \n",
    "        'recall': recall, \n",
    "        'f1': f1, \n",
    "        'top3_accuracy': top3_accuracy\n",
    "    }\n",
    "\n",
    "# Evaluate on all datasets\n",
    "train_metrics = evaluate_multiclass_model(y_train, y_train_pred, y_train_proba, \"Training\")\n",
    "val_metrics = evaluate_multiclass_model(y_val, y_val_pred, y_val_proba, \"Validation\")\n",
    "test_metrics = evaluate_multiclass_model(y_test, y_test_pred, y_test_proba, \"Test\")\n",
    "\n",
    "# Detailed classification report for test set\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"DETAILED CLASSIFICATION REPORT (Test Set)\")\n",
    "print(f\"=\"*80)\n",
    "\n",
    "class_names = [name[:30] + '...' if len(name) > 30 else name for name in label_encoder.classes_]\n",
    "report = classification_report(y_test, y_test_pred, \n",
    "                             target_names=class_names,\n",
    "                             digits=3, \n",
    "                             zero_division=0)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783f8c2a",
   "metadata": {},
   "source": [
    "## 9. Model Visualization\n",
    "\n",
    "Create visualizations to understand model performance and feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391b2dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-class model visualization\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "# 1. Feature Importance\n",
    "plt.subplot(2, 3, 1)\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': available_features,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=True)\n",
    "\n",
    "plt.barh(range(len(importance_df)), importance_df['importance'])\n",
    "plt.yticks(range(len(importance_df)), importance_df['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Feature Importance (Multi-Class)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Model Performance Comparison\n",
    "plt.subplot(2, 3, 2)\n",
    "metrics_comparison = pd.DataFrame({\n",
    "    'Train': [train_metrics['accuracy'], train_metrics['f1'], train_metrics['top3_accuracy']],\n",
    "    'Validation': [val_metrics['accuracy'], val_metrics['f1'], val_metrics['top3_accuracy']],\n",
    "    'Test': [test_metrics['accuracy'], test_metrics['f1'], test_metrics['top3_accuracy']]\n",
    "})\n",
    "metrics_comparison.index = ['Accuracy', 'F1-Score', 'Top-3 Accuracy']\n",
    "metrics_comparison.plot(kind='bar', ax=plt.gca())\n",
    "plt.title('Model Performance Across Splits')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Class Distribution in Predictions\n",
    "plt.subplot(2, 3, 3)\n",
    "pred_dist = pd.Series(y_test_pred).value_counts().sort_index()\n",
    "true_dist = pd.Series(y_test).value_counts().sort_index()\n",
    "\n",
    "# Show top 10 classes only for clarity\n",
    "top_10_classes = true_dist.head(10).index\n",
    "pred_counts = [pred_dist.get(i, 0) for i in top_10_classes]\n",
    "true_counts = [true_dist.get(i, 0) for i in top_10_classes]\n",
    "\n",
    "x = np.arange(len(top_10_classes))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, true_counts, width, label='True', alpha=0.8)\n",
    "plt.bar(x + width/2, pred_counts, width, label='Predicted', alpha=0.8)\n",
    "plt.xlabel('Crime Class (Top 10)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('True vs Predicted Class Distribution')\n",
    "plt.xticks(x, [f'C{i}' for i in top_10_classes], rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "# 4. Top-K Accuracy Analysis\n",
    "plt.subplot(2, 3, 4)\n",
    "k_values = range(1, min(11, len(label_encoder.classes_)))\n",
    "top_k_accuracies = []\n",
    "\n",
    "for k in k_values:\n",
    "    top_k_acc = 0\n",
    "    for i, (true_class, pred_probs) in enumerate(zip(y_test, y_test_proba)):\n",
    "        top_k_pred = np.argsort(pred_probs)[-k:]\n",
    "        if true_class in top_k_pred:\n",
    "            top_k_acc += 1\n",
    "    top_k_accuracies.append(top_k_acc / len(y_test))\n",
    "\n",
    "plt.plot(k_values, top_k_accuracies, marker='o', linewidth=2)\n",
    "plt.xlabel('K (Top-K Predictions)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Top-K Accuracy Analysis')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Per-Class Performance Heatmap\n",
    "plt.subplot(2, 3, 5)\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_per_class, recall_per_class, f1_per_class, support = precision_recall_fscore_support(\n",
    "    y_test, y_test_pred, average=None, zero_division=0\n",
    ")\n",
    "\n",
    "# Show top 15 classes with most support\n",
    "top_15_indices = np.argsort(support)[-15:]\n",
    "metrics_matrix = np.array([\n",
    "    precision_per_class[top_15_indices],\n",
    "    recall_per_class[top_15_indices],\n",
    "    f1_per_class[top_15_indices]\n",
    "])\n",
    "\n",
    "class_labels = [label_encoder.classes_[i][:20] + '...' if len(label_encoder.classes_[i]) > 20 \n",
    "                else label_encoder.classes_[i] for i in top_15_indices]\n",
    "\n",
    "sns.heatmap(metrics_matrix, \n",
    "            xticklabels=class_labels,\n",
    "            yticklabels=['Precision', 'Recall', 'F1-Score'],\n",
    "            annot=True, fmt='.2f', cmap='RdYlBu_r')\n",
    "plt.title('Per-Class Performance (Top 15)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# 6. Confusion Matrix (Top 10 Classes)\n",
    "plt.subplot(2, 3, 6)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Filter for top 10 classes for better visualization\n",
    "mask = np.isin(y_test, top_10_classes) & np.isin(y_test_pred, top_10_classes)\n",
    "y_test_filtered = y_test[mask]\n",
    "y_pred_filtered = y_test_pred[mask]\n",
    "\n",
    "if len(y_test_filtered) > 0:\n",
    "    cm = confusion_matrix(y_test_filtered, y_pred_filtered, \n",
    "                         labels=top_10_classes)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=[f'C{i}' for i in top_10_classes],\n",
    "                yticklabels=[f'C{i}' for i in top_10_classes])\n",
    "    plt.title('Confusion Matrix (Top 10 Classes)')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMulti-Class Model Summary:\")\n",
    "print(f\"‚úì Successfully trained model for {len(label_encoder.classes_)} crime categories\")\n",
    "print(f\"‚úì Test Accuracy: {test_metrics['accuracy']:.3f}\")\n",
    "print(f\"‚úì Test Top-3 Accuracy: {test_metrics['top3_accuracy']:.3f}\")\n",
    "print(f\"‚úì Weighted F1-Score: {test_metrics['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc59ae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed Multi-Class Analysis and Crime Type Insights\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "# 1. Crime Category Predictions Analysis\n",
    "plt.subplot(3, 3, 1)\n",
    "# Show prediction accuracy for top 10 crime types\n",
    "class_accuracies = []\n",
    "class_names_short = []\n",
    "top_10_indices = np.argsort(pd.Series(y_test).value_counts().values)[-10:]\n",
    "\n",
    "for class_idx in top_10_indices:\n",
    "    mask = (y_test == class_idx)\n",
    "    if np.sum(mask) > 0:\n",
    "        accuracy = np.mean(y_test_pred[mask] == class_idx)\n",
    "        class_accuracies.append(accuracy)\n",
    "        class_names_short.append(label_encoder.classes_[class_idx][:25] + '...' \n",
    "                                if len(label_encoder.classes_[class_idx]) > 25 \n",
    "                                else label_encoder.classes_[class_idx])\n",
    "\n",
    "plt.barh(range(len(class_accuracies)), class_accuracies, color='steelblue', alpha=0.8)\n",
    "plt.yticks(range(len(class_accuracies)), class_names_short)\n",
    "plt.xlabel('Per-Class Accuracy')\n",
    "plt.title('Accuracy by Crime Type (Top 10)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Prediction Confidence Distribution\n",
    "plt.subplot(3, 3, 2)\n",
    "max_probs = np.max(y_test_proba, axis=1)\n",
    "plt.hist(max_probs, bins=30, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "plt.xlabel('Prediction Confidence')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Model Confidence Distribution')\n",
    "plt.axvline(np.mean(max_probs), color='red', linestyle='--', \n",
    "           label=f'Mean: {np.mean(max_probs):.3f}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Most Confused Crime Pairs\n",
    "plt.subplot(3, 3, 3)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_full = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Find most confused pairs (off-diagonal elements)\n",
    "confusion_pairs = []\n",
    "for i in range(len(cm_full)):\n",
    "    for j in range(len(cm_full)):\n",
    "        if i != j and cm_full[i, j] > 0:\n",
    "            confusion_pairs.append((i, j, cm_full[i, j]))\n",
    "\n",
    "# Sort by confusion count and show top pairs\n",
    "confusion_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "top_confusions = confusion_pairs[:8]\n",
    "\n",
    "if top_confusions:\n",
    "    pairs_labels = []\n",
    "    confusion_counts = []\n",
    "    for true_idx, pred_idx, count in top_confusions:\n",
    "        true_name = label_encoder.classes_[true_idx][:15] + '...' if len(label_encoder.classes_[true_idx]) > 15 else label_encoder.classes_[true_idx]\n",
    "        pred_name = label_encoder.classes_[pred_idx][:15] + '...' if len(label_encoder.classes_[pred_idx]) > 15 else label_encoder.classes_[pred_idx]\n",
    "        pairs_labels.append(f\"{true_name}\\n‚Üí{pred_name}\")\n",
    "        confusion_counts.append(count)\n",
    "    \n",
    "    plt.barh(range(len(confusion_counts)), confusion_counts, color='salmon', alpha=0.8)\n",
    "    plt.yticks(range(len(confusion_counts)), pairs_labels)\n",
    "    plt.xlabel('Confusion Count')\n",
    "    plt.title('Most Confused Crime Pairs')\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "# 4. Feature Importance for Multi-Class\n",
    "plt.subplot(3, 3, 4)\n",
    "top_features = importance_df.tail(10)\n",
    "plt.bar(range(len(top_features)), top_features['importance'], color='darkgreen', alpha=0.8)\n",
    "plt.xticks(range(len(top_features)), top_features['feature'], rotation=45, ha='right')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Top 10 Most Important Features')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Crime Type Complexity Analysis\n",
    "plt.subplot(3, 3, 5)\n",
    "class_support = pd.Series(y_test).value_counts().sort_index()\n",
    "class_f1_scores = f1_per_class\n",
    "valid_indices = class_support.index[class_support > 10]  # Only classes with >10 samples\n",
    "\n",
    "if len(valid_indices) > 0:\n",
    "    support_vals = [class_support[i] for i in valid_indices]\n",
    "    f1_vals = [class_f1_scores[i] for i in valid_indices]\n",
    "    \n",
    "    plt.scatter(support_vals, f1_vals, alpha=0.6, s=50)\n",
    "    plt.xlabel('Class Support (Test Set)')\n",
    "    plt.ylabel('F1-Score')\n",
    "    plt.title('F1-Score vs Class Support')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(support_vals, f1_vals, 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(sorted(support_vals), p(sorted(support_vals)), \"r--\", alpha=0.8)\n",
    "\n",
    "# 6. Temporal Patterns in Predictions\n",
    "plt.subplot(3, 3, 6)\n",
    "# Analyze if certain crime types are predicted more in certain time periods\n",
    "if 'hour' in X_test.columns:\n",
    "    # Show prediction distribution by hour for top 3 crime types\n",
    "    top_3_classes = pd.Series(y_test_pred).value_counts().head(3).index\n",
    "    \n",
    "    for i, class_idx in enumerate(top_3_classes):\n",
    "        mask = (y_test_pred == class_idx)\n",
    "        if np.sum(mask) > 0:\n",
    "            hours = X_test.loc[mask, 'hour'].values\n",
    "            plt.hist(hours, bins=24, alpha=0.5, label=f\"Class {class_idx}\", density=True)\n",
    "    \n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Predicted Crime Types by Hour')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 7. Geographic Distribution of Predictions\n",
    "plt.subplot(3, 3, 7)\n",
    "if 'LAT' in X_test.columns and 'LON' in X_test.columns:\n",
    "    # Sample data for visualization\n",
    "    sample_size = min(2000, len(X_test))\n",
    "    sample_indices = np.random.choice(len(X_test), sample_size, replace=False)\n",
    "    \n",
    "    lat_sample = X_test.iloc[sample_indices]['LAT']\n",
    "    lon_sample = X_test.iloc[sample_indices]['LON']\n",
    "    pred_sample = y_test_pred[sample_indices]\n",
    "    \n",
    "    # Color by top 5 most common predicted classes\n",
    "    top_5_classes = pd.Series(pred_sample).value_counts().head(5).index\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, len(top_5_classes)))\n",
    "    \n",
    "    for i, class_idx in enumerate(top_5_classes):\n",
    "        mask = (pred_sample == class_idx)\n",
    "        if np.sum(mask) > 0:\n",
    "            plt.scatter(lon_sample[mask], lat_sample[mask], \n",
    "                       c=[colors[i]], alpha=0.6, s=10, \n",
    "                       label=f\"Class {class_idx}\")\n",
    "    \n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.title('Geographic Distribution of Predictions')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# 8. Model Calibration Analysis\n",
    "plt.subplot(3, 3, 8)\n",
    "# Analyze if predicted probabilities are well calibrated\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# For binary version: convert to \"most common class\" vs \"others\"\n",
    "most_common_class = pd.Series(y_test).value_counts().index[0]\n",
    "y_binary = (y_test == most_common_class).astype(int)\n",
    "y_prob_binary = y_test_proba[:, most_common_class]\n",
    "\n",
    "if len(np.unique(y_binary)) > 1:  # Only if both classes present\n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "        y_binary, y_prob_binary, n_bins=10)\n",
    "    \n",
    "    plt.plot(mean_predicted_value, fraction_of_positives, \"s-\", \n",
    "             label=\"Model\", color='blue')\n",
    "    plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfect calibration\")\n",
    "    plt.xlabel('Mean Predicted Probability')\n",
    "    plt.ylabel('Fraction of Positives')\n",
    "    plt.title('Calibration Curve (Most Common Class)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 9. Performance Summary\n",
    "plt.subplot(3, 3, 9)\n",
    "plt.axis('off')\n",
    "\n",
    "performance_summary = f\"\"\"\n",
    "MULTI-CLASS CRIME PREDICTION RESULTS\n",
    "\n",
    "Total Classes: {len(label_encoder.classes_)}\n",
    "Test Samples: {len(y_test):,}\n",
    "\n",
    "Performance Metrics:\n",
    "  Accuracy: {test_metrics['accuracy']:.3f}\n",
    "  Top-3 Accuracy: {test_metrics['top3_accuracy']:.3f}\n",
    "  Weighted F1: {test_metrics['f1']:.3f}\n",
    "  Weighted Precision: {test_metrics['precision']:.3f}\n",
    "  Weighted Recall: {test_metrics['recall']:.3f}\n",
    "\n",
    "Best Performing Features:\n",
    "  1. {importance_df.iloc[-1]['feature']}\n",
    "  2. {importance_df.iloc[-2]['feature']}\n",
    "  3. {importance_df.iloc[-3]['feature']}\n",
    "\n",
    "Average Confidence: {np.mean(max_probs):.3f}\n",
    "\"\"\"\n",
    "\n",
    "plt.text(0.1, 0.9, performance_summary, transform=plt.gca().transAxes, \n",
    "         fontsize=11, verticalalignment='top', fontfamily='monospace',\n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nDetailed Multi-Class Analysis Complete!\")\n",
    "print(f\"Model successfully predicts {len(label_encoder.classes_)} different crime types\")\n",
    "print(f\"This enables targeted crime prevention strategies for each specific crime category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb0bbde",
   "metadata": {},
   "source": [
    "## 10. Feature Analysis\n",
    "\n",
    "Analyze which features are most important for crime prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27764462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance ranking\n",
    "feature_ranking = importance_df.sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance Ranking:\")\n",
    "print(\"Rank  Feature                   Importance  Interpretation\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "interpretations = {\n",
    "    'hour': 'Time of day patterns',\n",
    "    'LAT': 'Geographic latitude hotspots',\n",
    "    'LON': 'Geographic longitude hotspots',\n",
    "    'day_of_week': 'Weekly crime patterns',\n",
    "    'month': 'Seasonal variations',\n",
    "    'AREA': 'Police area characteristics',\n",
    "    'Rpt Dist No': 'District-level patterns',\n",
    "    'Vict Age': 'Age demographic factors',\n",
    "    'year': 'Long-term trends',\n",
    "    'is_weekend': 'Weekend vs weekday',\n",
    "    'is_night': 'Night vs day patterns',\n",
    "    'Vict Sex_encoded': 'Gender patterns',\n",
    "    'Vict Descent_encoded': 'Demographic patterns',\n",
    "    'AREA NAME_encoded': 'Neighborhood factors'\n",
    "}\n",
    "\n",
    "for i, (_, row) in enumerate(feature_ranking.iterrows(), 1):\n",
    "    feature = row['feature']\n",
    "    importance = row['importance']\n",
    "    interp = interpretations.get(feature, 'Crime factor')\n",
    "    print(f\"{i:2d}.   {feature:<22} {importance:8.4f}    {interp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713b030d",
   "metadata": {},
   "source": [
    "## 11. Model Insights and Conclusions\n",
    "\n",
    "Summary of findings and practical applications for law enforcement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe0ef8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Multi-Class Crime Prediction Model - Final Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  Model Type: XGBoost Multi-Class Classifier\")\n",
    "print(f\"  Number of Crime Categories: {len(label_encoder.classes_)}\")\n",
    "print(f\"  Training Objective: Multi-class softmax probability\")\n",
    "print(f\"  Features Used: {len(available_features)}\")\n",
    "\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"  Test Accuracy: {test_metrics['accuracy']:.1%}\")\n",
    "print(f\"  Test Top-3 Accuracy: {test_metrics['top3_accuracy']:.1%}\")\n",
    "print(f\"  Weighted F1-Score: {test_metrics['f1']:.3f}\")\n",
    "print(f\"  Weighted Precision: {test_metrics['precision']:.3f}\")\n",
    "print(f\"  Weighted Recall: {test_metrics['recall']:.3f}\")\n",
    "\n",
    "print(f\"\\nTop Crime Categories Predicted:\")\n",
    "pred_distribution = pd.Series(y_test_pred).value_counts().head(5)\n",
    "for i, (class_idx, count) in enumerate(pred_distribution.items(), 1):\n",
    "    class_name = label_encoder.classes_[class_idx]\n",
    "    percentage = count / len(y_test_pred) * 100\n",
    "    print(f\"  {i}. {class_name:<45} {count:>5,} ({percentage:4.1f}%)\")\n",
    "\n",
    "print(f\"\\nMost Important Predictive Factors:\")\n",
    "top_5_features = importance_df.tail(5)\n",
    "for i, (_, row) in enumerate(reversed(list(top_5_features.iterrows())), 1):\n",
    "    print(f\"  {i}. {row['feature']:<25} (importance: {row['importance']:.3f})\")\n",
    "\n",
    "print(f\"\\nAdvantages of Multi-Class Approach:\")\n",
    "print(f\"  ‚úì Specific crime type predictions enable targeted interventions\")\n",
    "print(f\"  ‚úì Resource allocation can be optimized per crime category\")\n",
    "print(f\"  ‚úì Different patrol strategies for different crime types\")\n",
    "print(f\"  ‚úì Crime-specific temporal and geographic insights\")\n",
    "print(f\"  ‚úì Better understanding of crime patterns and relationships\")\n",
    "\n",
    "print(f\"\\nPractical Law Enforcement Applications:\")\n",
    "print(f\"  ‚Ä¢ Burglary Prevention: Deploy resources during predicted high-risk times/areas\")\n",
    "print(f\"  ‚Ä¢ Vehicle Theft: Focus patrols in parking areas during peak prediction periods\")\n",
    "print(f\"  ‚Ä¢ Assault Cases: Increase presence in nightlife areas during high-risk hours\")\n",
    "print(f\"  ‚Ä¢ Drug Offenses: Target known hotspots with specialized units\")\n",
    "print(f\"  ‚Ä¢ Property Crimes: Coordinate with community watch programs\")\n",
    "\n",
    "print(f\"\\nModel Limitations & Considerations:\")\n",
    "print(f\"  ‚ö† Predictions are probabilistic estimates, not certainties\")\n",
    "print(f\"  ‚ö† Model performance varies by crime type frequency\")\n",
    "print(f\"  ‚ö† Requires regular retraining with new crime data\")\n",
    "print(f\"  ‚ö† Should complement, not replace, officer judgment\")\n",
    "print(f\"  ‚ö† Ethical considerations regarding bias and fairness\")\n",
    "\n",
    "print(f\"\\nNext Steps for Deployment:\")\n",
    "print(f\"  1. Validate model with domain experts (law enforcement)\")\n",
    "print(f\"  2. Implement real-time prediction pipeline\")\n",
    "print(f\"  3. Create interactive dashboard for police departments\")\n",
    "print(f\"  4. Set up monitoring and alert systems\")\n",
    "print(f\"  5. Establish feedback loop for continuous improvement\")\n",
    "print(f\"  6. Conduct field trials and validation studies\")\n",
    "\n",
    "# Create a summary of class performance\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"CRIME CATEGORY PERFORMANCE SUMMARY\")\n",
    "print(f\"=\"*80)\n",
    "\n",
    "# Calculate per-class metrics\n",
    "class_metrics = []\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    mask_true = (y_test == i)\n",
    "    mask_pred = (y_test_pred == i)\n",
    "    \n",
    "    if np.sum(mask_true) > 0:  # Only if class exists in test set\n",
    "        true_positives = np.sum((y_test == i) & (y_test_pred == i))\n",
    "        precision = true_positives / np.sum(mask_pred) if np.sum(mask_pred) > 0 else 0\n",
    "        recall = true_positives / np.sum(mask_true)\n",
    "        support = np.sum(mask_true)\n",
    "        \n",
    "        class_metrics.append({\n",
    "            'class': class_name[:40] + '...' if len(class_name) > 40 else class_name,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'support': support\n",
    "        })\n",
    "\n",
    "# Sort by support and show top performing classes\n",
    "class_metrics.sort(key=lambda x: x['support'], reverse=True)\n",
    "print(f\"{'Crime Type':<45} {'Precision':<10} {'Recall':<10} {'Support':<10}\")\n",
    "print(f\"{'-'*45} {'-'*10} {'-'*10} {'-'*10}\")\n",
    "\n",
    "for metric in class_metrics[:15]:  # Show top 15\n",
    "    print(f\"{metric['class']:<45} {metric['precision']:<10.3f} {metric['recall']:<10.3f} {metric['support']:<10,}\")\n",
    "\n",
    "print(f\"\\nThis multi-class model provides actionable insights for {len(label_encoder.classes_)} specific crime types,\")\n",
    "print(f\"enabling law enforcement to develop targeted prevention and response strategies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb60b51",
   "metadata": {},
   "source": [
    "## 2.5. Comprehensive Exploratory Data Analysis (EDA)\n",
    "\n",
    "This section provides in-depth analysis of crime patterns, distributions, and relationships in the Los Angeles crime data to guide our modeling approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656e046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Overview and Summary Statistics\n",
    "print(\"=== DATASET OVERVIEW ===\")\n",
    "print(f\"Total Records: {len(df):,}\")\n",
    "print(f\"Total Features: {len(df.columns)}\")\n",
    "print(f\"Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Data types and basic statistics\n",
    "print(f\"\\nData Types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "print(f\"\\nNumerical Features Summary:\")\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "print(df[numerical_cols].describe())\n",
    "\n",
    "# Check for class distribution\n",
    "print(f\"\\nTarget Variable Analysis:\")\n",
    "if 'Part 1-2' in df.columns:\n",
    "    part_dist = df['Part 1-2'].value_counts()\n",
    "    print(f\"Part 1 (Serious Crimes): {part_dist.get(1, 0):,}\")\n",
    "    print(f\"Part 2 (Less Serious): {part_dist.get(2, 0):,}\")\n",
    "    \n",
    "print(f\"\\nMost Common Crime Types:\")\n",
    "if 'Crm Cd Desc' in df.columns:\n",
    "    print(df['Crm Cd Desc'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c0daa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal Analysis - Crime Patterns Over Time\n",
    "print(\"=== TEMPORAL ANALYSIS ===\")\n",
    "\n",
    "# Parse temporal features for analysis\n",
    "df_temp = df.copy()\n",
    "df_temp['Date Rptd'] = pd.to_datetime(df_temp['Date Rptd'], errors='coerce')\n",
    "df_temp['DATE OCC'] = pd.to_datetime(df_temp['DATE OCC'], errors='coerce')\n",
    "\n",
    "# Extract time components\n",
    "df_temp['year'] = df_temp['DATE OCC'].dt.year\n",
    "df_temp['month'] = df_temp['DATE OCC'].dt.month\n",
    "df_temp['day_of_week'] = df_temp['DATE OCC'].dt.dayofweek\n",
    "df_temp['hour'] = pd.to_numeric(df_temp['TIME OCC'].astype(str).str[:2], errors='coerce')\n",
    "\n",
    "# Fill missing hours\n",
    "df_temp['hour'] = df_temp['hour'].fillna(df_temp['hour'].median())\n",
    "\n",
    "plt.figure(figsize=(18, 12))\n",
    "\n",
    "# 1. Crimes by Hour\n",
    "plt.subplot(3, 3, 1)\n",
    "hourly_crimes = df_temp.groupby('hour').size()\n",
    "hourly_crimes.plot(kind='line', marker='o')\n",
    "plt.title('Crime Distribution by Hour of Day')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Number of Crimes')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Crimes by Day of Week\n",
    "plt.subplot(3, 3, 2)\n",
    "daily_crimes = df_temp.groupby('day_of_week').size()\n",
    "day_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "plt.bar(range(7), daily_crimes.values)\n",
    "plt.title('Crime Distribution by Day of Week')\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Number of Crimes')\n",
    "plt.xticks(range(7), day_names)\n",
    "\n",
    "# 3. Crimes by Month\n",
    "plt.subplot(3, 3, 3)\n",
    "monthly_crimes = df_temp.groupby('month').size()\n",
    "monthly_crimes.plot(kind='bar')\n",
    "plt.title('Crime Distribution by Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Crimes')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# 4. Crimes by Year\n",
    "plt.subplot(3, 3, 4)\n",
    "if len(df_temp['year'].unique()) > 1:\n",
    "    yearly_crimes = df_temp.groupby('year').size()\n",
    "    yearly_crimes.plot(kind='line', marker='o')\n",
    "    plt.title('Crime Trends by Year')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Number of Crimes')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "# 5. Weekend vs Weekday\n",
    "plt.subplot(3, 3, 5)\n",
    "weekend_crimes = df_temp[df_temp['day_of_week'].isin([5, 6])].shape[0]\n",
    "weekday_crimes = df_temp[~df_temp['day_of_week'].isin([5, 6])].shape[0]\n",
    "plt.pie([weekday_crimes, weekend_crimes], labels=['Weekday', 'Weekend'], autopct='%1.1f%%')\n",
    "plt.title('Weekday vs Weekend Crimes')\n",
    "\n",
    "# 6. Peak Hours Analysis\n",
    "plt.subplot(3, 3, 6)\n",
    "peak_hours = hourly_crimes.nlargest(6)\n",
    "plt.bar(peak_hours.index, peak_hours.values, color='red', alpha=0.7)\n",
    "plt.title('Top 6 Crime Peak Hours')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Number of Crimes')\n",
    "\n",
    "# 7. Hourly Crime Heatmap by Day\n",
    "plt.subplot(3, 3, 7)\n",
    "if not df_temp.empty:\n",
    "    heatmap_data = df_temp.groupby(['day_of_week', 'hour']).size().unstack(fill_value=0)\n",
    "    sns.heatmap(heatmap_data, cmap='YlOrRd', cbar_kws={'label': 'Number of Crimes'})\n",
    "    plt.title('Crime Heatmap: Day vs Hour')\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel('Day of Week')\n",
    "    plt.yticks(range(7), day_names, rotation=0)\n",
    "\n",
    "# 8. Monthly trend with Part 1-2\n",
    "plt.subplot(3, 3, 8)\n",
    "if 'Part 1-2' in df_temp.columns:\n",
    "    monthly_part = df_temp.groupby(['month', 'Part 1-2']).size().unstack(fill_value=0)\n",
    "    monthly_part.plot(kind='bar', stacked=True)\n",
    "    plt.title('Monthly Crimes by Severity (Part 1-2)')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Number of Crimes')\n",
    "    plt.legend(['Part 1 (Serious)', 'Part 2 (Less Serious)'])\n",
    "    plt.xticks(rotation=0)\n",
    "\n",
    "# 9. Time series plot\n",
    "plt.subplot(3, 3, 9)\n",
    "if len(df_temp) > 0:\n",
    "    df_temp['date_only'] = df_temp['DATE OCC'].dt.date\n",
    "    daily_counts = df_temp.groupby('date_only').size()\n",
    "    if len(daily_counts) > 30:  # Only if we have enough data\n",
    "        daily_counts.tail(90).plot()  # Last 90 days\n",
    "        plt.title('Recent Daily Crime Counts (Last 90 days)')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Daily Crime Count')\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print temporal insights\n",
    "print(f\"\\nTemporal Insights:\")\n",
    "print(f\"Peak Crime Hour: {hourly_crimes.idxmax()}:00 ({hourly_crimes.max():,} crimes)\")\n",
    "print(f\"Lowest Crime Hour: {hourly_crimes.idxmin()}:00 ({hourly_crimes.min():,} crimes)\")\n",
    "print(f\"Peak Crime Day: {day_names[daily_crimes.idxmax()]} ({daily_crimes.max():,} crimes)\")\n",
    "print(f\"Peak Crime Month: {monthly_crimes.idxmax()} ({monthly_crimes.max():,} crimes)\")\n",
    "print(f\"Weekend Crime Percentage: {weekend_crimes/(weekend_crimes+weekday_crimes)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b91935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geographic Analysis - Crime Distribution by Location\n",
    "print(\"=== GEOGRAPHIC ANALYSIS ===\")\n",
    "\n",
    "# Filter valid coordinates\n",
    "df_geo = df[(df['LAT'] != 0) & (df['LON'] != 0)]\n",
    "df_geo = df_geo[(df_geo['LAT'] > 33) & (df_geo['LAT'] < 35)]\n",
    "df_geo = df_geo[(df_geo['LON'] > -119) & (df_geo['LON'] < -117)]\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "# 1. Overall Crime Distribution Map\n",
    "plt.subplot(2, 4, 1)\n",
    "plt.scatter(df_geo['LON'], df_geo['LAT'], alpha=0.1, s=0.1, c='red')\n",
    "plt.title('Crime Geographic Distribution')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Crime Density Heatmap\n",
    "plt.subplot(2, 4, 2)\n",
    "plt.hist2d(df_geo['LON'], df_geo['LAT'], bins=50, cmap='YlOrRd')\n",
    "plt.title('Crime Density Heatmap')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.colorbar(label='Crime Count')\n",
    "\n",
    "# 3. Crimes by Area\n",
    "plt.subplot(2, 4, 3)\n",
    "if 'AREA' in df.columns:\n",
    "    area_crimes = df['AREA'].value_counts().head(10)\n",
    "    area_crimes.plot(kind='bar')\n",
    "    plt.title('Top 10 Areas by Crime Count')\n",
    "    plt.xlabel('Area Code')\n",
    "    plt.ylabel('Crime Count')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "# 4. Crimes by Area Name\n",
    "plt.subplot(2, 4, 4)\n",
    "if 'AREA NAME' in df.columns:\n",
    "    area_name_crimes = df['AREA NAME'].value_counts().head(8)\n",
    "    plt.barh(range(len(area_name_crimes)), area_name_crimes.values)\n",
    "    plt.yticks(range(len(area_name_crimes)), area_name_crimes.index)\n",
    "    plt.title('Top 8 Areas by Name')\n",
    "    plt.xlabel('Crime Count')\n",
    "\n",
    "# 5. High Risk vs Low Risk Geographic Distribution\n",
    "plt.subplot(2, 4, 5)\n",
    "if 'Part 1-2' in df_geo.columns:\n",
    "    high_risk = df_geo[df_geo['Part 1-2'] == 1]\n",
    "    low_risk = df_geo[df_geo['Part 1-2'] == 2]\n",
    "    \n",
    "    plt.scatter(low_risk['LON'], low_risk['LAT'], alpha=0.3, s=0.5, c='blue', label='Part 2 (Less Serious)')\n",
    "    plt.scatter(high_risk['LON'], high_risk['LAT'], alpha=0.3, s=0.5, c='red', label='Part 1 (Serious)')\n",
    "    plt.title('Crime Risk Distribution')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.legend()\n",
    "\n",
    "# 6. Reporting Districts\n",
    "plt.subplot(2, 4, 6)\n",
    "if 'Rpt Dist No' in df.columns:\n",
    "    district_crimes = df['Rpt Dist No'].value_counts().head(10)\n",
    "    district_crimes.plot(kind='bar')\n",
    "    plt.title('Top 10 Reporting Districts')\n",
    "    plt.xlabel('District Number')\n",
    "    plt.ylabel('Crime Count')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "# 7. Premise Analysis\n",
    "plt.subplot(2, 4, 7)\n",
    "if 'Premis Desc' in df.columns:\n",
    "    premise_crimes = df['Premis Desc'].value_counts().head(8)\n",
    "    plt.pie(premise_crimes.values, labels=premise_crimes.index, autopct='%1.1f%%')\n",
    "    plt.title('Crime Distribution by Premise Type')\n",
    "\n",
    "# 8. Geographic Stats Summary\n",
    "plt.subplot(2, 4, 8)\n",
    "plt.axis('off')\n",
    "geographic_stats = f\"\"\"\n",
    "GEOGRAPHIC STATISTICS\n",
    "\n",
    "Total Valid Coordinates: {len(df_geo):,}\n",
    "\n",
    "Latitude Range:\n",
    "  Min: {df_geo['LAT'].min():.4f}\n",
    "  Max: {df_geo['LAT'].max():.4f}\n",
    "  \n",
    "Longitude Range:\n",
    "  Min: {df_geo['LON'].min():.4f}\n",
    "  Max: {df_geo['LON'].max():.4f}\n",
    "\n",
    "Most Crime-Prone Area:\n",
    "  {df['AREA NAME'].value_counts().index[0] if 'AREA NAME' in df.columns else 'N/A'}\n",
    "  ({df['AREA NAME'].value_counts().iloc[0]:,} crimes)\n",
    "\n",
    "Least Crime Area:\n",
    "  {df['AREA NAME'].value_counts().index[-1] if 'AREA NAME' in df.columns else 'N/A'}\n",
    "  ({df['AREA NAME'].value_counts().iloc[-1]:,} crimes)\n",
    "\"\"\"\n",
    "\n",
    "plt.text(0.1, 0.9, geographic_stats, transform=plt.gca().transAxes, \n",
    "         fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\"))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Geographic insights\n",
    "print(f\"\\nGeographic Insights:\")\n",
    "if 'AREA NAME' in df.columns:\n",
    "    area_stats = df['AREA NAME'].value_counts()\n",
    "    print(f\"Highest Crime Area: {area_stats.index[0]} ({area_stats.iloc[0]:,} crimes)\")\n",
    "    print(f\"Total Areas: {len(area_stats)}\")\n",
    "    print(f\"Average Crimes per Area: {area_stats.mean():.0f}\")\n",
    "    print(f\"Crime Concentration: Top 5 areas account for {area_stats.head(5).sum()/area_stats.sum()*100:.1f}% of all crimes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eacbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demographic Analysis - Victim Profiles and Patterns\n",
    "print(\"=== DEMOGRAPHIC ANALYSIS ===\")\n",
    "\n",
    "plt.figure(figsize=(18, 12))\n",
    "\n",
    "# 1. Victim Age Distribution\n",
    "plt.subplot(3, 4, 1)\n",
    "if 'Vict Age' in df.columns:\n",
    "    victim_ages = pd.to_numeric(df['Vict Age'], errors='coerce')\n",
    "    victim_ages = victim_ages[(victim_ages > 0) & (victim_ages < 100)]  # Filter realistic ages\n",
    "    \n",
    "    plt.hist(victim_ages, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    plt.title('Victim Age Distribution')\n",
    "    plt.xlabel('Age')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.axvline(victim_ages.mean(), color='red', linestyle='--', label=f'Mean: {victim_ages.mean():.1f}')\n",
    "    plt.legend()\n",
    "\n",
    "# 2. Victim Sex Distribution\n",
    "plt.subplot(3, 4, 2)\n",
    "if 'Vict Sex' in df.columns:\n",
    "    sex_counts = df['Vict Sex'].value_counts().head(6)\n",
    "    plt.pie(sex_counts.values, labels=sex_counts.index, autopct='%1.1f%%')\n",
    "    plt.title('Victim Gender Distribution')\n",
    "\n",
    "# 3. Victim Descent Distribution\n",
    "plt.subplot(3, 4, 3)\n",
    "if 'Vict Descent' in df.columns:\n",
    "    descent_counts = df['Vict Descent'].value_counts().head(8)\n",
    "    plt.bar(range(len(descent_counts)), descent_counts.values)\n",
    "    plt.title('Victim Ethnicity Distribution')\n",
    "    plt.xlabel('Ethnicity Code')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(range(len(descent_counts)), descent_counts.index, rotation=45)\n",
    "\n",
    "# 4. Age vs Crime Type (Part 1-2)\n",
    "plt.subplot(3, 4, 4)\n",
    "if 'Vict Age' in df.columns and 'Part 1-2' in df.columns:\n",
    "    victim_ages = pd.to_numeric(df['Vict Age'], errors='coerce')\n",
    "    valid_data = df[(victim_ages > 0) & (victim_ages < 100)]\n",
    "    \n",
    "    part1_ages = valid_data[valid_data['Part 1-2'] == 1]['Vict Age']\n",
    "    part2_ages = valid_data[valid_data['Part 1-2'] == 2]['Vict Age']\n",
    "    \n",
    "    plt.hist([part2_ages, part1_ages], bins=20, alpha=0.7, label=['Part 2 (Less Serious)', 'Part 1 (Serious)'])\n",
    "    plt.title('Age Distribution by Crime Severity')\n",
    "    plt.xlabel('Age')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "\n",
    "# 5. Gender vs Crime Type\n",
    "plt.subplot(3, 4, 5)\n",
    "if 'Vict Sex' in df.columns and 'Part 1-2' in df.columns:\n",
    "    gender_crime = pd.crosstab(df['Vict Sex'], df['Part 1-2'])\n",
    "    gender_crime.plot(kind='bar')\n",
    "    plt.title('Crime Severity by Gender')\n",
    "    plt.xlabel('Gender')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(['Part 1 (Serious)', 'Part 2 (Less Serious)'])\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "# 6. Age Groups Analysis\n",
    "plt.subplot(3, 4, 6)\n",
    "if 'Vict Age' in df.columns:\n",
    "    victim_ages = pd.to_numeric(df['Vict Age'], errors='coerce')\n",
    "    valid_ages = victim_ages[(victim_ages > 0) & (victim_ages < 100)]\n",
    "    \n",
    "    age_groups = pd.cut(valid_ages, bins=[0, 18, 25, 35, 50, 65, 100], \n",
    "                       labels=['0-17', '18-24', '25-34', '35-49', '50-64', '65+'])\n",
    "    age_group_counts = age_groups.value_counts().sort_index()\n",
    "    \n",
    "    plt.bar(range(len(age_group_counts)), age_group_counts.values, color='lightcoral')\n",
    "    plt.title('Victims by Age Groups')\n",
    "    plt.xlabel('Age Group')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(range(len(age_group_counts)), age_group_counts.index, rotation=45)\n",
    "\n",
    "# 7. Hour vs Gender\n",
    "plt.subplot(3, 4, 7)\n",
    "if 'TIME OCC' in df.columns and 'Vict Sex' in df.columns:\n",
    "    df_temp['hour'] = pd.to_numeric(df_temp['TIME OCC'].astype(str).str[:2], errors='coerce')\n",
    "    df_temp['hour'] = df_temp['hour'].fillna(df_temp['hour'].median())\n",
    "    \n",
    "    gender_hour = df_temp.groupby(['hour', 'Vict Sex']).size().unstack(fill_value=0)\n",
    "    top_genders = df['Vict Sex'].value_counts().head(3).index\n",
    "    \n",
    "    for gender in top_genders:\n",
    "        if gender in gender_hour.columns:\n",
    "            plt.plot(gender_hour.index, gender_hour[gender], label=gender, marker='o', markersize=3)\n",
    "    \n",
    "    plt.title('Crime Patterns by Hour and Gender')\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel('Crime Count')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 8. Demographic Summary Stats\n",
    "plt.subplot(3, 4, 8)\n",
    "plt.axis('off')\n",
    "\n",
    "demo_stats = \"\"\n",
    "if 'Vict Age' in df.columns:\n",
    "    ages = pd.to_numeric(df['Vict Age'], errors='coerce')\n",
    "    valid_ages = ages[(ages > 0) & (ages < 100)]\n",
    "    demo_stats += f\"Age Statistics:\\n  Mean: {valid_ages.mean():.1f}\\n  Median: {valid_ages.median():.1f}\\n  Most Common: {valid_ages.mode().iloc[0] if len(valid_ages.mode()) > 0 else 'N/A'}\\n\\n\"\n",
    "\n",
    "if 'Vict Sex' in df.columns:\n",
    "    gender_stats = df['Vict Sex'].value_counts()\n",
    "    demo_stats += f\"Gender Distribution:\\n\"\n",
    "    for gender, count in gender_stats.head(3).items():\n",
    "        demo_stats += f\"  {gender}: {count:,} ({count/len(df)*100:.1f}%)\\n\"\n",
    "\n",
    "demo_stats += f\"\\nTotal Victims Analyzed: {len(df):,}\"\n",
    "\n",
    "plt.text(0.1, 0.9, demo_stats, transform=plt.gca().transAxes, \n",
    "         fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightyellow\"))\n",
    "\n",
    "# 9-12: Additional demographic plots\n",
    "plt.subplot(3, 4, 9)\n",
    "if 'Vict Sex' in df.columns and 'Vict Age' in df.columns:\n",
    "    # Age boxplot by gender\n",
    "    valid_df = df[pd.to_numeric(df['Vict Age'], errors='coerce').between(0, 100)]\n",
    "    top_genders = valid_df['Vict Sex'].value_counts().head(3).index\n",
    "    \n",
    "    age_by_gender = [pd.to_numeric(valid_df[valid_df['Vict Sex'] == gender]['Vict Age'], errors='coerce').dropna() \n",
    "                     for gender in top_genders]\n",
    "    \n",
    "    plt.boxplot(age_by_gender, labels=top_genders)\n",
    "    plt.title('Age Distribution by Gender')\n",
    "    plt.xlabel('Gender')\n",
    "    plt.ylabel('Age')\n",
    "\n",
    "plt.subplot(3, 4, 10)\n",
    "if 'Vict Descent' in df.columns and 'Part 1-2' in df.columns:\n",
    "    # Ethnicity vs Crime Severity\n",
    "    descent_crime = pd.crosstab(df['Vict Descent'], df['Part 1-2'])\n",
    "    descent_crime.head(6).plot(kind='bar', stacked=True)\n",
    "    plt.title('Crime Severity by Ethnicity (Top 6)')\n",
    "    plt.xlabel('Ethnicity')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(['Part 1', 'Part 2'])\n",
    "\n",
    "plt.subplot(3, 4, 11)\n",
    "# Age vs Hour correlation\n",
    "if 'Vict Age' in df.columns and 'TIME OCC' in df.columns:\n",
    "    df_temp['age'] = pd.to_numeric(df_temp['Vict Age'], errors='coerce')\n",
    "    valid_temp = df_temp[(df_temp['age'] > 0) & (df_temp['age'] < 100)]\n",
    "    \n",
    "    plt.scatter(valid_temp['hour'], valid_temp['age'], alpha=0.1, s=1)\n",
    "    plt.title('Age vs Hour of Crime')\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel('Age')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(3, 4, 12)\n",
    "# Crime count by demographics\n",
    "if 'Vict Sex' in df.columns and 'Vict Descent' in df.columns:\n",
    "    demo_combo = df.groupby(['Vict Sex', 'Vict Descent']).size().reset_index(name='count')\n",
    "    top_combos = demo_combo.nlargest(8, 'count')\n",
    "    \n",
    "    labels = [f\"{row['Vict Sex']}-{row['Vict Descent']}\" for _, row in top_combos.iterrows()]\n",
    "    plt.barh(range(len(top_combos)), top_combos['count'])\n",
    "    plt.yticks(range(len(top_combos)), labels)\n",
    "    plt.title('Top Victim Demographics')\n",
    "    plt.xlabel('Crime Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print demographic insights\n",
    "print(f\"\\nDemographic Insights:\")\n",
    "if 'Vict Age' in df.columns:\n",
    "    ages = pd.to_numeric(df['Vict Age'], errors='coerce')\n",
    "    valid_ages = ages[(ages > 0) & (ages < 100)]\n",
    "    print(f\"Average Victim Age: {valid_ages.mean():.1f} years\")\n",
    "    print(f\"Most Vulnerable Age Group: {pd.cut(valid_ages, bins=[0,18,25,35,50,65,100]).value_counts().idxmax()}\")\n",
    "\n",
    "if 'Vict Sex' in df.columns:\n",
    "    gender_stats = df['Vict Sex'].value_counts()\n",
    "    print(f\"Most Affected Gender: {gender_stats.index[0]} ({gender_stats.iloc[0]:,} cases, {gender_stats.iloc[0]/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a4b311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crime Type and Severity Analysis\n",
    "print(\"=== CRIME TYPE AND SEVERITY ANALYSIS ===\")\n",
    "\n",
    "plt.figure(figsize=(18, 10))\n",
    "\n",
    "# 1. Crime Code Distribution\n",
    "plt.subplot(2, 4, 1)\n",
    "if 'Crm Cd' in df.columns:\n",
    "    crime_codes = df['Crm Cd'].value_counts().head(10)\n",
    "    plt.bar(range(len(crime_codes)), crime_codes.values)\n",
    "    plt.title('Top 10 Crime Codes')\n",
    "    plt.xlabel('Crime Code Rank')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(range(len(crime_codes)), [f'{int(code)}' for code in crime_codes.index], rotation=45)\n",
    "\n",
    "# 2. Crime Description\n",
    "plt.subplot(2, 4, 2)\n",
    "if 'Crm Cd Desc' in df.columns:\n",
    "    crime_desc = df['Crm Cd Desc'].value_counts().head(8)\n",
    "    plt.barh(range(len(crime_desc)), crime_desc.values)\n",
    "    plt.yticks(range(len(crime_desc)), [desc[:20] + '...' if len(desc) > 20 else desc for desc in crime_desc.index])\n",
    "    plt.title('Top Crime Types')\n",
    "    plt.xlabel('Count')\n",
    "\n",
    "# 3. Part 1 vs Part 2 Distribution\n",
    "plt.subplot(2, 4, 3)\n",
    "if 'Part 1-2' in df.columns:\n",
    "    part_dist = df['Part 1-2'].value_counts()\n",
    "    labels = ['Part 1 (Serious)', 'Part 2 (Less Serious)']\n",
    "    colors = ['red', 'orange']\n",
    "    plt.pie(part_dist.values, labels=labels, colors=colors, autopct='%1.1f%%')\n",
    "    plt.title('Crime Severity Distribution')\n",
    "\n",
    "# 4. Status Analysis\n",
    "plt.subplot(2, 4, 4)\n",
    "if 'Status Desc' in df.columns:\n",
    "    status_counts = df['Status Desc'].value_counts().head(8)\n",
    "    plt.bar(range(len(status_counts)), status_counts.values, color='lightblue')\n",
    "    plt.title('Case Status Distribution')\n",
    "    plt.xlabel('Status Type')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(range(len(status_counts)), [s[:10] for s in status_counts.index], rotation=45)\n",
    "\n",
    "# 5. Weapon Usage\n",
    "plt.subplot(2, 4, 5)\n",
    "if 'Weapon Desc' in df.columns:\n",
    "    weapon_counts = df['Weapon Desc'].value_counts().head(8)\n",
    "    plt.barh(range(len(weapon_counts)), weapon_counts.values, color='darkred')\n",
    "    plt.yticks(range(len(weapon_counts)), [w[:15] for w in weapon_counts.index])\n",
    "    plt.title('Weapon Types Used')\n",
    "    plt.xlabel('Count')\n",
    "\n",
    "# 6. Crime Severity by Hour\n",
    "plt.subplot(2, 4, 6)\n",
    "if 'Part 1-2' in df.columns and 'TIME OCC' in df.columns:\n",
    "    df_temp['hour'] = pd.to_numeric(df_temp['TIME OCC'].astype(str).str[:2], errors='coerce')\n",
    "    df_temp['hour'] = df_temp['hour'].fillna(df_temp['hour'].median())\n",
    "    \n",
    "    hourly_severity = df_temp.groupby(['hour', 'Part 1-2']).size().unstack(fill_value=0)\n",
    "    hourly_severity.plot(kind='line', ax=plt.gca(), color=['red', 'orange'])\n",
    "    plt.title('Crime Severity Patterns by Hour')\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel('Crime Count')\n",
    "    plt.legend(['Part 1 (Serious)', 'Part 2 (Less Serious)'])\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 7. MO (Modus Operandi) Analysis\n",
    "plt.subplot(2, 4, 7)\n",
    "if 'Mocodes' in df.columns:\n",
    "    mo_counts = df[df['Mocodes'].notna()]['Mocodes'].value_counts().head(8)\n",
    "    if len(mo_counts) > 0:\n",
    "        plt.bar(range(len(mo_counts)), mo_counts.values, color='purple', alpha=0.7)\n",
    "        plt.title('Modus Operandi Patterns')\n",
    "        plt.xlabel('MO Code')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(range(len(mo_counts)), mo_counts.index, rotation=45)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'No MO Data Available', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Modus Operandi Patterns')\n",
    "\n",
    "# 8. Crime Trends Summary\n",
    "plt.subplot(2, 4, 8)\n",
    "plt.axis('off')\n",
    "\n",
    "crime_summary = f\"\"\"\n",
    "CRIME TYPE SUMMARY\n",
    "\n",
    "Total Crime Records: {len(df):,}\n",
    "\"\"\"\n",
    "\n",
    "if 'Part 1-2' in df.columns:\n",
    "    part_counts = df['Part 1-2'].value_counts()\n",
    "    crime_summary += f\"\"\"\n",
    "Severity Breakdown:\n",
    "  Part 1 (Serious): {part_counts.get(1, 0):,} ({part_counts.get(1, 0)/len(df)*100:.1f}%)\n",
    "  Part 2 (Less Serious): {part_counts.get(2, 0):,} ({part_counts.get(2, 0)/len(df)*100:.1f}%)\n",
    "\"\"\"\n",
    "\n",
    "if 'Crm Cd Desc' in df.columns:\n",
    "    top_crime = df['Crm Cd Desc'].value_counts().iloc[0]\n",
    "    crime_summary += f\"\"\"\n",
    "Most Common Crime:\n",
    "  {df['Crm Cd Desc'].value_counts().index[0][:25]}...\n",
    "  ({top_crime:,} cases)\n",
    "\"\"\"\n",
    "\n",
    "if 'Status Desc' in df.columns:\n",
    "    status_stats = df['Status Desc'].value_counts()\n",
    "    crime_summary += f\"\"\"\n",
    "Case Resolution:\n",
    "  {status_stats.index[0]}: {status_stats.iloc[0]:,} cases\n",
    "  Resolution Rate: {(status_stats.get('INVESTIGATED', 0) + status_stats.get('CLOSED', 0))/len(df)*100:.1f}%\n",
    "\"\"\"\n",
    "\n",
    "plt.text(0.1, 0.9, crime_summary, transform=plt.gca().transAxes, \n",
    "         fontsize=9, verticalalignment='top', fontfamily='monospace',\n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightcyan\"))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print crime type insights\n",
    "print(f\"\\nCrime Type Insights:\")\n",
    "if 'Crm Cd Desc' in df.columns:\n",
    "    crime_types = df['Crm Cd Desc'].value_counts()\n",
    "    print(f\"Most Common Crime: {crime_types.index[0]} ({crime_types.iloc[0]:,} cases)\")\n",
    "    print(f\"Total Crime Types: {len(crime_types)}\")\n",
    "    print(f\"Crime Concentration: Top 10 crimes represent {crime_types.head(10).sum()/crime_types.sum()*100:.1f}% of all cases\")\n",
    "\n",
    "if 'Part 1-2' in df.columns:\n",
    "    part_stats = df['Part 1-2'].value_counts()\n",
    "    print(f\"Serious Crime Rate (Part 1): {part_stats.get(1, 0)/len(df)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d440c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis and Feature Relationships\n",
    "print(\"=== CORRELATION ANALYSIS ===\")\n",
    "\n",
    "# Prepare numerical features for correlation analysis\n",
    "numerical_features = ['LAT', 'LON', 'AREA', 'Rpt Dist No', 'Vict Age']\n",
    "\n",
    "# Add time features if they exist\n",
    "if 'DATE OCC' in df.columns:\n",
    "    df_corr = df.copy()\n",
    "    df_corr['DATE OCC'] = pd.to_datetime(df_corr['DATE OCC'], errors='coerce')\n",
    "    df_corr['year'] = df_corr['DATE OCC'].dt.year\n",
    "    df_corr['month'] = df_corr['DATE OCC'].dt.month\n",
    "    df_corr['day_of_week'] = df_corr['DATE OCC'].dt.dayofweek\n",
    "    df_corr['hour'] = pd.to_numeric(df_corr['TIME OCC'].astype(str).str[:2], errors='coerce')\n",
    "    df_corr['hour'] = df_corr['hour'].fillna(df_corr['hour'].median())\n",
    "    \n",
    "    numerical_features.extend(['year', 'month', 'day_of_week', 'hour'])\n",
    "\n",
    "# Add target variable\n",
    "if 'Part 1-2' in df.columns:\n",
    "    df_corr['high_risk'] = (df_corr['Part 1-2'] == 1).astype(int)\n",
    "    numerical_features.append('high_risk')\n",
    "\n",
    "# Select available numerical features\n",
    "available_numerical = [col for col in numerical_features if col in df_corr.columns]\n",
    "\n",
    "# Clean numerical data\n",
    "df_numerical = df_corr[available_numerical].copy()\n",
    "for col in df_numerical.columns:\n",
    "    if col not in ['high_risk']:  # Don't modify target variable\n",
    "        df_numerical[col] = pd.to_numeric(df_numerical[col], errors='coerce')\n",
    "\n",
    "# Fill missing values with median\n",
    "df_numerical = df_numerical.fillna(df_numerical.median())\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# 1. Correlation Matrix Heatmap\n",
    "plt.subplot(2, 3, 1)\n",
    "correlation_matrix = df_numerical.corr()\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='RdYlBu_r', center=0, \n",
    "            square=True, fmt='.2f', cbar_kws={'shrink': 0.8})\n",
    "plt.title('Feature Correlation Matrix')\n",
    "\n",
    "# 2. High Risk Correlation\n",
    "plt.subplot(2, 3, 2)\n",
    "if 'high_risk' in df_numerical.columns:\n",
    "    risk_correlations = correlation_matrix['high_risk'].drop('high_risk').abs().sort_values(ascending=False)\n",
    "    plt.bar(range(len(risk_correlations)), risk_correlations.values, color='crimson', alpha=0.7)\n",
    "    plt.title('Features Correlated with High Risk')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Absolute Correlation')\n",
    "    plt.xticks(range(len(risk_correlations)), risk_correlations.index, rotation=45)\n",
    "\n",
    "# 3. Time vs Geographic Correlation\n",
    "plt.subplot(2, 3, 3)\n",
    "if all(col in df_numerical.columns for col in ['hour', 'LAT', 'LON']):\n",
    "    plt.scatter(df_numerical['hour'], df_numerical['LAT'], alpha=0.1, s=1, c='blue', label='Latitude')\n",
    "    plt.scatter(df_numerical['hour'], df_numerical['LON'], alpha=0.1, s=1, c='red', label='Longitude')\n",
    "    plt.title('Time vs Geographic Distribution')\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel('Coordinate Value')\n",
    "    plt.legend()\n",
    "\n",
    "# 4. Age vs Risk Analysis\n",
    "plt.subplot(2, 3, 4)\n",
    "if all(col in df_numerical.columns for col in ['Vict Age', 'high_risk']):\n",
    "    valid_ages = df_numerical[(df_numerical['Vict Age'] > 0) & (df_numerical['Vict Age'] < 100)]\n",
    "    \n",
    "    low_risk_ages = valid_ages[valid_ages['high_risk'] == 0]['Vict Age']\n",
    "    high_risk_ages = valid_ages[valid_ages['high_risk'] == 1]['Vict Age']\n",
    "    \n",
    "    plt.boxplot([low_risk_ages, high_risk_ages], labels=['Low Risk', 'High Risk'])\n",
    "    plt.title('Victim Age by Risk Level')\n",
    "    plt.ylabel('Age')\n",
    "\n",
    "# 5. Geographic Risk Patterns\n",
    "plt.subplot(2, 3, 5)\n",
    "if all(col in df_numerical.columns for col in ['LAT', 'LON', 'high_risk']):\n",
    "    high_risk_crimes = df_numerical[df_numerical['high_risk'] == 1]\n",
    "    low_risk_crimes = df_numerical[df_numerical['high_risk'] == 0]\n",
    "    \n",
    "    # Sample data for better visualization\n",
    "    if len(high_risk_crimes) > 5000:\n",
    "        high_risk_sample = high_risk_crimes.sample(5000, random_state=42)\n",
    "        low_risk_sample = low_risk_crimes.sample(5000, random_state=42)\n",
    "    else:\n",
    "        high_risk_sample = high_risk_crimes\n",
    "        low_risk_sample = low_risk_crimes\n",
    "    \n",
    "    plt.scatter(low_risk_sample['LON'], low_risk_sample['LAT'], \n",
    "               alpha=0.3, s=1, c='blue', label='Low Risk')\n",
    "    plt.scatter(high_risk_sample['LON'], high_risk_sample['LAT'], \n",
    "               alpha=0.5, s=1, c='red', label='High Risk')\n",
    "    plt.title('Geographic Risk Distribution')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.legend()\n",
    "\n",
    "# 6. Correlation Summary Stats\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.axis('off')\n",
    "\n",
    "corr_summary = \"CORRELATION INSIGHTS\\n\\n\"\n",
    "\n",
    "if 'high_risk' in correlation_matrix.columns:\n",
    "    risk_corr = correlation_matrix['high_risk'].drop('high_risk').abs().sort_values(ascending=False)\n",
    "    corr_summary += f\"Strongest Risk Predictors:\\n\"\n",
    "    for i, (feature, corr_val) in enumerate(risk_corr.head(5).items()):\n",
    "        corr_summary += f\"  {i+1}. {feature}: {corr_val:.3f}\\n\"\n",
    "    \n",
    "    corr_summary += f\"\\nWeakest Risk Predictors:\\n\"\n",
    "    for i, (feature, corr_val) in enumerate(risk_corr.tail(3).items()):\n",
    "        corr_summary += f\"  {feature}: {corr_val:.3f}\\n\"\n",
    "\n",
    "# Find highly correlated feature pairs\n",
    "high_corr_pairs = []\n",
    "for i, col1 in enumerate(correlation_matrix.columns):\n",
    "    for j, col2 in enumerate(correlation_matrix.columns):\n",
    "        if i < j and abs(correlation_matrix.iloc[i, j]) > 0.7:\n",
    "            high_corr_pairs.append((col1, col2, correlation_matrix.iloc[i, j]))\n",
    "\n",
    "if high_corr_pairs:\n",
    "    corr_summary += f\"\\nHighly Correlated Pairs (>0.7):\\n\"\n",
    "    for col1, col2, corr_val in high_corr_pairs[:3]:\n",
    "        corr_summary += f\"  {col1} ‚Üî {col2}: {corr_val:.3f}\\n\"\n",
    "\n",
    "plt.text(0.1, 0.9, corr_summary, transform=plt.gca().transAxes, \n",
    "         fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\", alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print correlation insights\n",
    "print(f\"\\nCorrelation Analysis Results:\")\n",
    "print(f\"Total numerical features analyzed: {len(available_numerical)}\")\n",
    "\n",
    "if 'high_risk' in correlation_matrix.columns:\n",
    "    risk_correlations = correlation_matrix['high_risk'].drop('high_risk').abs().sort_values(ascending=False)\n",
    "    print(f\"\\nStrongest predictors of high-risk crimes:\")\n",
    "    for i, (feature, corr) in enumerate(risk_correlations.head(3).items(), 1):\n",
    "        print(f\"  {i}. {feature}: {corr:.3f} correlation\")\n",
    "    \n",
    "    print(f\"\\nFeature pairs with high correlation (potential multicollinearity):\")\n",
    "    for col1, col2, corr_val in high_corr_pairs[:3]:\n",
    "        print(f\"  {col1} ‚Üî {col2}: {corr_val:.3f}\")\n",
    "\n",
    "# Statistical significance tests (if scipy available)\n",
    "try:\n",
    "    from scipy.stats import chi2_contingency\n",
    "    \n",
    "    # Test independence between categorical variables\n",
    "    if all(col in df.columns for col in ['Vict Sex', 'Part 1-2']):\n",
    "        contingency_table = pd.crosstab(df['Vict Sex'], df['Part 1-2'])\n",
    "        chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "        print(f\"\\nChi-square test (Gender vs Crime Severity):\")\n",
    "        print(f\"  p-value: {p_value:.4f} ({'Significant' if p_value < 0.05 else 'Not significant'})\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"\\nScipy not available for statistical significance tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188b5174",
   "metadata": {},
   "source": [
    "## EDA Summary and Key Insights\n",
    "\n",
    "### Target Variable: High Risk Crime Classification\n",
    "- **Dependent Variable**: `high_risk` (binary: 1 for Part I crimes, 0 for Part II crimes)\n",
    "- **Independent Variables**: Temporal, Geographic, and Demographic features\n",
    "\n",
    "### Key Findings from Exploratory Data Analysis:\n",
    "\n",
    "**Temporal Patterns:**\n",
    "- Peak crime hours: Evening and late night periods show highest activity\n",
    "- Weekly patterns: Certain days show higher crime concentration  \n",
    "- Seasonal trends: Monthly variations in crime frequency and types\n",
    "- Weekend vs weekday differences in crime patterns\n",
    "\n",
    "**Geographic Distribution:**  \n",
    "- Crime hotspots concentrate in specific LAT/LON coordinates\n",
    "- Area codes show distinct crime density patterns\n",
    "- Spatial clustering of high-risk vs low-risk crimes\n",
    "- Geographic correlation with demographic factors\n",
    "\n",
    "**Demographic Insights:**\n",
    "- Age distribution varies by crime severity level\n",
    "- Gender patterns differ between crime types\n",
    "- Victim demographics correlate with crime risk levels\n",
    "- Age-risk relationships show distinct patterns\n",
    "\n",
    "**Crime Type Analysis:**\n",
    "- Part I crimes (high-risk) vs Part II crimes show different characteristics\n",
    "- Weapon usage patterns vary by crime severity\n",
    "- Crime status distribution indicates resolution patterns\n",
    "- Modus operandi patterns provide behavioral insights\n",
    "\n",
    "**Feature Relationships:**\n",
    "- Strong correlations between temporal and geographic variables\n",
    "- Age and location show interaction effects\n",
    "- Multicollinearity considerations for model building\n",
    "- Feature importance varies by crime type\n",
    "\n",
    "### Data Quality Assessment:\n",
    "- Missing value patterns identified and addressed\n",
    "- Outlier detection completed for key variables\n",
    "- Data distribution characteristics documented\n",
    "- Feature engineering opportunities identified\n",
    "\n",
    "This comprehensive EDA provides the foundation for building predictive models to classify high-risk crimes based on temporal, geographic, and demographic patterns in Los Angeles crime data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe645d0",
   "metadata": {},
   "source": [
    "## District/Area-Wise Crime Distribution Analysis\n",
    "\n",
    "Comprehensive analysis of crime distribution across Los Angeles police districts and areas for strategic resource allocation and targeted law enforcement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b15c825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === COMPREHENSIVE DISTRICT/AREA-WISE CRIME ANALYSIS ===\n",
    "print(\"=\"*80)\n",
    "print(\"         COMPLETE LOS ANGELES DISTRICT CRIME DISTRIBUTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check available district/area columns\n",
    "district_columns = ['AREA NAME', 'AREA', 'Rpt Dist No']\n",
    "available_district_cols = [col for col in district_columns if col in df.columns]\n",
    "print(f\"Available district information: {available_district_cols}\")\n",
    "\n",
    "# 1. AREA NAME Analysis (Police Districts/Divisions)\n",
    "if 'AREA NAME' in df.columns:\n",
    "    area_name_stats = df['AREA NAME'].value_counts().sort_values(ascending=False)\n",
    "    \n",
    "    print(f\"\\nüìç TOTAL LA POLICE DISTRICTS (AREA NAMES): {len(area_name_stats)}\")\n",
    "    print(f\"üìä TOTAL CRIME RECORDS: {len(df):,}\")\n",
    "    print(f\"üìà AVERAGE CRIMES PER DISTRICT: {area_name_stats.mean():.0f}\")\n",
    "    print(f\"üìâ MEDIAN CRIMES PER DISTRICT: {area_name_stats.median():.0f}\")\n",
    "    print(f\"üî• HIGHEST CRIME DISTRICT: {area_name_stats.index[0]} ({area_name_stats.iloc[0]:,} crimes)\")\n",
    "    print(f\"üü¢ LOWEST CRIME DISTRICT: {area_name_stats.index[-1]} ({area_name_stats.iloc[-1]:,} crimes)\")\n",
    "    \n",
    "    print(f\"\\nüèÜ ALL DISTRICTS RANKED BY CRIME COUNT:\")\n",
    "    print(\"-\" * 70)\n",
    "    for i, (district, count) in enumerate(area_name_stats.items(), 1):\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"{i:2d}. {district:<35} {count:>8,} crimes ({percentage:>5.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nüìä DISTRICT CRIME CONCENTRATION:\")\n",
    "    top_5_crimes = area_name_stats.head(5).sum()\n",
    "    top_10_crimes = area_name_stats.head(10).sum()\n",
    "    top_15_crimes = area_name_stats.head(15).sum()\n",
    "    \n",
    "    print(f\"   Top 5 districts:  {top_5_crimes:>8,} crimes ({top_5_crimes/len(df)*100:>5.1f}%)\")\n",
    "    print(f\"   Top 10 districts: {top_10_crimes:>8,} crimes ({top_10_crimes/len(df)*100:>5.1f}%)\")\n",
    "    print(f\"   Top 15 districts: {top_15_crimes:>8,} crimes ({top_15_crimes/len(df)*100:>5.1f}%)\")\n",
    "\n",
    "# 2. AREA Code Analysis (Numeric Area Codes)\n",
    "if 'AREA' in df.columns:\n",
    "    area_code_stats = df['AREA'].value_counts().sort_values(ascending=False)\n",
    "    \n",
    "    print(f\"\\nüìç TOTAL AREA CODES: {len(area_code_stats)}\")\n",
    "    print(f\"üìà AVERAGE CRIMES PER AREA CODE: {area_code_stats.mean():.0f}\")\n",
    "    \n",
    "    print(f\"\\nüî• TOP 20 AREA CODES BY CRIME COUNT:\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, (area_code, count) in enumerate(area_code_stats.head(20).items(), 1):\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"{i:2d}. Area Code {area_code:<8} {count:>8,} crimes ({percentage:>5.1f}%)\")\n",
    "\n",
    "# 3. Reporting Districts Analysis (More granular)\n",
    "if 'Rpt Dist No' in df.columns:\n",
    "    rpt_dist_stats = df['Rpt Dist No'].value_counts().sort_values(ascending=False)\n",
    "    \n",
    "    print(f\"\\nüìç TOTAL REPORTING DISTRICTS: {len(rpt_dist_stats)}\")\n",
    "    print(f\"üìà AVERAGE CRIMES PER REPORTING DISTRICT: {rpt_dist_stats.mean():.0f}\")\n",
    "    \n",
    "    print(f\"\\nüî• TOP 20 REPORTING DISTRICTS BY CRIME COUNT:\")\n",
    "    print(\"-\" * 60)\n",
    "    for i, (district, count) in enumerate(rpt_dist_stats.head(20).items(), 1):\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"{i:2d}. Reporting District {district:<8} {count:>8,} crimes ({percentage:>5.1f}%)\")\n",
    "\n",
    "# Create comprehensive visualization\n",
    "plt.figure(figsize=(25, 20))\n",
    "\n",
    "# 1. All Districts Crime Distribution (AREA NAME)\n",
    "plt.subplot(4, 4, 1)\n",
    "if 'AREA NAME' in df.columns:\n",
    "    all_districts = df['AREA NAME'].value_counts()\n",
    "    colors = plt.cm.RdYlBu_r(np.linspace(0.2, 0.8, len(all_districts)))\n",
    "    plt.barh(range(len(all_districts)), all_districts.values, color=colors)\n",
    "    plt.yticks(range(len(all_districts)), [name[:20] + '...' if len(name) > 20 else name for name in all_districts.index])\n",
    "    plt.xlabel('Crime Count')\n",
    "    plt.title(f'ALL {len(all_districts)} LA POLICE DISTRICTS\\n(AREA NAMES)')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Top 15 Districts Detail\n",
    "plt.subplot(4, 4, 2)\n",
    "if 'AREA NAME' in df.columns:\n",
    "    top_15 = df['AREA NAME'].value_counts().head(15)\n",
    "    colors = plt.cm.Reds(np.linspace(0.3, 1, len(top_15)))\n",
    "    bars = plt.barh(range(len(top_15)), top_15.values, color=colors)\n",
    "    plt.yticks(range(len(top_15)), [name[:18] for name in top_15.index])\n",
    "    plt.xlabel('Crime Count')\n",
    "    plt.title('TOP 15 DISTRICTS (Detailed)')\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        plt.text(width + 100, bar.get_y() + bar.get_height()/2, \n",
    "                f'{int(width):,}', ha='left', va='center', fontsize=8)\n",
    "\n",
    "# 3. District Crime Share (Pie Chart)\n",
    "plt.subplot(4, 4, 3)\n",
    "if 'AREA NAME' in df.columns:\n",
    "    district_counts = df['AREA NAME'].value_counts()\n",
    "    top_10 = district_counts.head(10)\n",
    "    others = district_counts.iloc[10:].sum()\n",
    "    \n",
    "    plot_data = list(top_10.values) + [others]\n",
    "    plot_labels = list(top_10.index) + [f'Others ({len(district_counts)-10} districts)']\n",
    "    \n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(plot_data)))\n",
    "    wedges, texts, autotexts = plt.pie(plot_data, labels=[label[:12] + '...' if len(label) > 12 else label for label in plot_labels], \n",
    "                                      autopct='%1.1f%%', startangle=90, colors=colors)\n",
    "    plt.title('District Crime Share\\n(Top 10 + Others)')\n",
    "    \n",
    "    # Improve text readability\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_weight('bold')\n",
    "\n",
    "# 4. Area Code Distribution\n",
    "plt.subplot(4, 4, 4)\n",
    "if 'AREA' in df.columns:\n",
    "    top_area_codes = df['AREA'].value_counts().head(20)\n",
    "    plt.bar(range(len(top_area_codes)), top_area_codes.values, color='orange', alpha=0.8)\n",
    "    plt.xlabel('Area Code Rank')\n",
    "    plt.ylabel('Crime Count')\n",
    "    plt.title('TOP 20 AREA CODES')\n",
    "    plt.xticks(range(0, len(top_area_codes), 3), \n",
    "               [f'#{i+1}' for i in range(0, len(top_area_codes), 3)], rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Crime Density Distribution\n",
    "plt.subplot(4, 4, 5)\n",
    "if 'AREA NAME' in df.columns:\n",
    "    district_crimes = df['AREA NAME'].value_counts().values\n",
    "    plt.hist(district_crimes, bins=15, color='skyblue', alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('Crime Count per District')\n",
    "    plt.ylabel('Number of Districts')\n",
    "    plt.title('Crime Distribution Across Districts')\n",
    "    plt.axvline(np.mean(district_crimes), color='red', linestyle='--', \n",
    "                label=f'Mean: {np.mean(district_crimes):.0f}')\n",
    "    plt.axvline(np.median(district_crimes), color='green', linestyle='--', \n",
    "                label=f'Median: {np.median(district_crimes):.0f}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. District vs Crime Category (Heatmap)\n",
    "plt.subplot(4, 4, 6)\n",
    "if 'AREA NAME' in df.columns and 'crime_category' in df.columns:\n",
    "    top_10_districts = df['AREA NAME'].value_counts().head(10).index\n",
    "    top_8_crimes = df['crime_category'].value_counts().head(8).index\n",
    "    \n",
    "    district_crime_crosstab = pd.crosstab(\n",
    "        df[df['AREA NAME'].isin(top_10_districts)]['AREA NAME'], \n",
    "        df[df['AREA NAME'].isin(top_10_districts)]['crime_category']\n",
    "    )\n",
    "    \n",
    "    # Filter for top crimes\n",
    "    available_crimes = [crime for crime in top_8_crimes if crime in district_crime_crosstab.columns]\n",
    "    if available_crimes:\n",
    "        district_crime_subset = district_crime_crosstab[available_crimes]\n",
    "        \n",
    "        sns.heatmap(district_crime_subset, annot=True, fmt='d', cmap='YlOrRd', \n",
    "                   cbar_kws={'label': 'Crime Count'})\n",
    "        plt.title('Top 10 Districts vs Crime Types')\n",
    "        plt.xlabel('Crime Category')\n",
    "        plt.ylabel('District')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "\n",
    "# 7. Reporting Districts Analysis\n",
    "plt.subplot(4, 4, 7)\n",
    "if 'Rpt Dist No' in df.columns:\n",
    "    top_rpt_districts = df['Rpt Dist No'].value_counts().head(25)\n",
    "    plt.scatter(range(len(top_rpt_districts)), top_rpt_districts.values, \n",
    "               alpha=0.7, s=60, c=top_rpt_districts.values, cmap='viridis')\n",
    "    plt.xlabel('Reporting District Rank')\n",
    "    plt.ylabel('Crime Count')\n",
    "    plt.title('TOP 25 REPORTING DISTRICTS')\n",
    "    plt.colorbar(label='Crime Count')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 8. District Ranking Analysis\n",
    "plt.subplot(4, 4, 8)\n",
    "if 'AREA NAME' in df.columns:\n",
    "    district_ranking = df['AREA NAME'].value_counts()\n",
    "    rankings = range(1, len(district_ranking) + 1)\n",
    "    \n",
    "    plt.scatter(rankings, district_ranking.values, alpha=0.6, s=50, c='purple')\n",
    "    plt.xlabel('District Rank')\n",
    "    plt.ylabel('Crime Count (Log Scale)')\n",
    "    plt.title('District Crime Count by Rank')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Annotate top 3\n",
    "    for i, (district, count) in enumerate(district_ranking.head(3).items()):\n",
    "        plt.annotate(f'{district[:10]}...', (i+1, count), xytext=(5, 5), \n",
    "                    textcoords='offset points', fontsize=8, \n",
    "                    bbox=dict(boxstyle=\"round,pad=0.2\", facecolor=\"yellow\", alpha=0.7))\n",
    "\n",
    "# 9. Crime Temporal Patterns by District\n",
    "plt.subplot(4, 4, 9)\n",
    "if 'AREA NAME' in df.columns and 'hour' in df.columns:\n",
    "    top_5_districts = df['AREA NAME'].value_counts().head(5).index\n",
    "    district_hourly = df[df['AREA NAME'].isin(top_5_districts)].groupby(['AREA NAME', 'hour']).size().unstack(fill_value=0)\n",
    "    \n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(top_5_districts)))\n",
    "    for i, district in enumerate(top_5_districts):\n",
    "        if district in district_hourly.index:\n",
    "            plt.plot(district_hourly.columns, district_hourly.loc[district], \n",
    "                    label=district[:12] + '...', marker='o', markersize=3, \n",
    "                    color=colors[i], linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Crime Count')\n",
    "    plt.title('Hourly Patterns (Top 5 Districts)')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 10. Geographic Distribution\n",
    "plt.subplot(4, 4, 10)\n",
    "if 'LAT' in df.columns and 'LON' in df.columns and 'AREA NAME' in df.columns:\n",
    "    # Sample data for visualization\n",
    "    sample_size = min(5000, len(df))\n",
    "    sample_df = df.sample(sample_size, random_state=42)\n",
    "    \n",
    "    top_districts = df['AREA NAME'].value_counts().head(5).index\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, len(top_districts)))\n",
    "    \n",
    "    for i, district in enumerate(top_districts):\n",
    "        district_data = sample_df[sample_df['AREA NAME'] == district]\n",
    "        if len(district_data) > 0:\n",
    "            plt.scatter(district_data['LON'], district_data['LAT'], \n",
    "                       c=[colors[i]], alpha=0.6, s=15, \n",
    "                       label=district[:15] + '...')\n",
    "    \n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.title('Geographic Distribution by District')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 11. District Performance Metrics\n",
    "plt.subplot(4, 4, 11)\n",
    "if 'AREA NAME' in df.columns and 'Part 1-2' in df.columns:\n",
    "    district_severity = df.groupby('AREA NAME')['Part 1-2'].agg(['count', 'mean']).reset_index()\n",
    "    district_severity.columns = ['District', 'Total_Crimes', 'Severity_Ratio']\n",
    "    district_severity = district_severity.sort_values('Total_Crimes', ascending=False).head(15)\n",
    "    \n",
    "    plt.scatter(district_severity['Total_Crimes'], district_severity['Severity_Ratio'], \n",
    "               s=100, alpha=0.7, c='red')\n",
    "    \n",
    "    for i, row in district_severity.iterrows():\n",
    "        plt.annotate(row['District'][:8] + '...', \n",
    "                    (row['Total_Crimes'], row['Severity_Ratio']), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    plt.xlabel('Total Crime Count')\n",
    "    plt.ylabel('Severity Ratio (Part 1 Crimes)')\n",
    "    plt.title('District Crime Volume vs Severity')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 12. Area Code vs District Mapping\n",
    "plt.subplot(4, 4, 12)\n",
    "if 'AREA' in df.columns and 'AREA NAME' in df.columns:\n",
    "    area_district_mapping = df.groupby(['AREA', 'AREA NAME']).size().reset_index(name='count')\n",
    "    top_area_district = area_district_mapping.nlargest(15, 'count')\n",
    "    \n",
    "    plt.barh(range(len(top_area_district)), top_area_district['count'], color='green', alpha=0.7)\n",
    "    labels = [f\"Area {row['AREA']}: {row['AREA NAME'][:15]}...\" for _, row in top_area_district.iterrows()]\n",
    "    plt.yticks(range(len(top_area_district)), labels)\n",
    "    plt.xlabel('Crime Count')\n",
    "    plt.title('Top 15 Area-District Combinations')\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "# 13-16: Summary Statistics and Additional Analysis\n",
    "plt.subplot(4, 4, 13)\n",
    "plt.axis('off')\n",
    "\n",
    "if 'AREA NAME' in df.columns:\n",
    "    district_stats = df['AREA NAME'].value_counts()\n",
    "    \n",
    "    stats_summary = f\"\"\"\n",
    "LA POLICE DISTRICT SUMMARY\n",
    "\n",
    "Total Districts: {len(district_stats)}\n",
    "Total Crimes: {len(df):,}\n",
    "\n",
    "Crime Statistics:\n",
    "‚îú‚îÄ Highest: {district_stats.iloc[0]:,} crimes\n",
    "‚îú‚îÄ Lowest: {district_stats.iloc[-1]:,} crimes  \n",
    "‚îú‚îÄ Average: {district_stats.mean():.0f} crimes\n",
    "‚îú‚îÄ Median: {district_stats.median():.0f} crimes\n",
    "‚îî‚îÄ Std Dev: {district_stats.std():.0f} crimes\n",
    "\n",
    "District Workload Distribution:\n",
    "‚îú‚îÄ Top 25% districts handle:\n",
    "‚îÇ   {district_stats.head(len(district_stats)//4).sum()/len(df)*100:.1f}% of crimes\n",
    "‚îú‚îÄ Top 50% districts handle:\n",
    "‚îÇ   {district_stats.head(len(district_stats)//2).sum()/len(df)*100:.1f}% of crimes\n",
    "‚îî‚îÄ Bottom 25% districts handle:\n",
    "    {district_stats.tail(len(district_stats)//4).sum()/len(df)*100:.1f}% of crimes\n",
    "\n",
    "Workload Inequality:\n",
    "‚îî‚îÄ Ratio (Highest/Lowest): \n",
    "    {district_stats.iloc[0]/district_stats.iloc[-1]:.1f}x difference\n",
    "\"\"\"\n",
    "\n",
    "    plt.text(0.05, 0.95, stats_summary, transform=plt.gca().transAxes, \n",
    "             fontsize=9, verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle=\"round,pad=0.4\", facecolor=\"lightblue\", alpha=0.8))\n",
    "\n",
    "# 14. Resource Allocation Priority\n",
    "plt.subplot(4, 4, 14)\n",
    "if 'AREA NAME' in df.columns:\n",
    "    district_counts = df['AREA NAME'].value_counts()\n",
    "    \n",
    "    # Categorize districts by crime volume\n",
    "    high_priority = district_counts[district_counts > district_counts.quantile(0.8)]\n",
    "    medium_priority = district_counts[(district_counts >= district_counts.quantile(0.4)) & \n",
    "                                    (district_counts <= district_counts.quantile(0.8))]\n",
    "    low_priority = district_counts[district_counts < district_counts.quantile(0.4)]\n",
    "    \n",
    "    categories = ['High Priority\\n(Top 20%)', 'Medium Priority\\n(Middle 40%)', 'Low Priority\\n(Bottom 40%)']\n",
    "    counts = [len(high_priority), len(medium_priority), len(low_priority)]\n",
    "    colors = ['red', 'orange', 'green']\n",
    "    \n",
    "    plt.pie(counts, labels=categories, autopct='%1.0f%%', colors=colors, startangle=90)\n",
    "    plt.title('Resource Allocation Priority\\n(District Categories)')\n",
    "\n",
    "# 15. Crime Trend Analysis\n",
    "plt.subplot(4, 4, 15)\n",
    "if 'AREA NAME' in df.columns and 'year' in df.columns and len(df['year'].unique()) > 1:\n",
    "    top_5_districts = df['AREA NAME'].value_counts().head(5).index\n",
    "    yearly_trends = df[df['AREA NAME'].isin(top_5_districts)].groupby(['AREA NAME', 'year']).size().unstack(fill_value=0)\n",
    "    \n",
    "    for district in top_5_districts:\n",
    "        if district in yearly_trends.index:\n",
    "            plt.plot(yearly_trends.columns, yearly_trends.loc[district], \n",
    "                    label=district[:12] + '...', marker='o', linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Crime Count')\n",
    "    plt.title('Crime Trends by District (Top 5)')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'Insufficient temporal data\\nfor trend analysis', \n",
    "             ha='center', va='center', transform=plt.gca().transAxes,\n",
    "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\"))\n",
    "    plt.title('Crime Trends Analysis')\n",
    "\n",
    "# 16. Key Insights Summary\n",
    "plt.subplot(4, 4, 16)\n",
    "plt.axis('off')\n",
    "\n",
    "insights_text = \"\"\"\n",
    "KEY INSIGHTS & RECOMMENDATIONS\n",
    "\n",
    "üéØ FOCUS AREAS (Why Top 15 Analysis):\n",
    "‚Ä¢ Pareto Principle applies to crime distribution\n",
    "‚Ä¢ Top 15 districts handle majority of crimes\n",
    "‚Ä¢ Limited resources require strategic focus\n",
    "‚Ä¢ Statistical significance in high-crime areas\n",
    "\n",
    "üìä RESOURCE ALLOCATION:\n",
    "‚Ä¢ HIGH PRIORITY: Top 20% districts\n",
    "  - Enhanced patrol presence\n",
    "  - Specialized crime units\n",
    "  - Community engagement programs\n",
    "\n",
    "‚Ä¢ MEDIUM PRIORITY: Middle 40% districts  \n",
    "  - Regular patrol schedules\n",
    "  - Crime prevention programs\n",
    "  - Inter-district coordination\n",
    "\n",
    "‚Ä¢ LOW PRIORITY: Bottom 40% districts\n",
    "  - Maintenance patrols\n",
    "  - Community policing focus\n",
    "  - Crime prevention education\n",
    "\n",
    "üöî OPERATIONAL BENEFITS:\n",
    "‚Ä¢ Targeted deployment strategies\n",
    "‚Ä¢ Data-driven resource allocation  \n",
    "‚Ä¢ Performance benchmarking\n",
    "‚Ä¢ Inter-district comparison\n",
    "\"\"\"\n",
    "\n",
    "plt.text(0.05, 0.95, insights_text, transform=plt.gca().transAxes, \n",
    "         fontsize=9, verticalalignment='top',\n",
    "         bbox=dict(boxstyle=\"round,pad=0.4\", facecolor=\"lightgreen\", alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"STRATEGIC INSIGHTS FOR LAW ENFORCEMENT:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'AREA NAME' in df.columns:\n",
    "    district_stats = df['AREA NAME'].value_counts()\n",
    "    \n",
    "    print(f\"üéØ RESOURCE ALLOCATION PRIORITIES:\")\n",
    "    \n",
    "    # High priority districts\n",
    "    high_priority = district_stats[district_stats > district_stats.quantile(0.8)]\n",
    "    print(f\"\\nüî¥ HIGH PRIORITY DISTRICTS ({len(high_priority)} districts):\")\n",
    "    print(f\"   Handle {high_priority.sum()/len(df)*100:.1f}% of all crimes\")\n",
    "    for i, (district, count) in enumerate(high_priority.head(10).items(), 1):\n",
    "        print(f\"   {i:2d}. {district:<30} {count:>6,} crimes\")\n",
    "    \n",
    "    # Medium priority districts  \n",
    "    medium_priority = district_stats[(district_stats >= district_stats.quantile(0.4)) & \n",
    "                                   (district_stats <= district_stats.quantile(0.8))]\n",
    "    print(f\"\\nüü° MEDIUM PRIORITY DISTRICTS ({len(medium_priority)} districts):\")\n",
    "    print(f\"   Handle {medium_priority.sum()/len(df)*100:.1f}% of all crimes\")\n",
    "    \n",
    "    # Low priority districts\n",
    "    low_priority = district_stats[district_stats < district_stats.quantile(0.4)]\n",
    "    print(f\"\\nüü¢ LOW PRIORITY DISTRICTS ({len(low_priority)} districts):\")\n",
    "    print(f\"   Handle {low_priority.sum()/len(df)*100:.1f}% of all crimes\")\n",
    "    \n",
    "    print(f\"\\nüìà ACTIONABLE RECOMMENDATIONS:\")\n",
    "    print(f\"   ‚Ä¢ Deploy 60% of resources to HIGH priority districts\")\n",
    "    print(f\"   ‚Ä¢ Deploy 30% of resources to MEDIUM priority districts\")\n",
    "    print(f\"   ‚Ä¢ Deploy 10% of resources to LOW priority districts\")\n",
    "    print(f\"   ‚Ä¢ Focus predictive modeling on top 15-20 districts\")\n",
    "    print(f\"   ‚Ä¢ Implement district-specific crime prevention strategies\")\n",
    "    print(f\"   ‚Ä¢ Establish inter-district resource sharing protocols\")\n",
    "\n",
    "print(f\"\\n‚úÖ DISTRICT ANALYSIS COMPLETE!\")\n",
    "print(f\"This comprehensive analysis enables data-driven police resource allocation and strategic planning.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crime-prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
